{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рекомендательные системы - это модели классификации, где целевая переменная - уровень интереса, а в качестве признаков используются данные по пользователю $x_{user}$ и товару $x_{item}$, а также (опционально) набор других признаков $x_{context}$.\n",
    "\n",
    "Поэтому начнем обзор с моделей классификации.\n",
    "\n",
    "# Модели\n",
    "\n",
    "В наиболее общей формулировке модель машинного обучения выгллядит так:\n",
    "\n",
    "$$y=f(x)$$\n",
    "где y - целевая переменная, а x - вектор признаков.\n",
    "\n",
    "Чаще всего ограничиваются классом линейных моделей:\n",
    "\n",
    "$$y=w_0+w_1x_1+\\cdots+w_nx_n$$ \n",
    "\n",
    "Далее для краткости я буду обозначать данную линейную комбинацию в виде скалярного произведения двух векторов $x$ и $w$ (свободный коэффициент $w_0$ также включаем в вектор w):\n",
    "\n",
    "$$\\hat{y}=\\langle w,x \\rangle$$\n",
    "\n",
    "Для того чтобы модель лучше отслеживала нелинейные признаковые зависимости, можем в число признаков добавить попарные взаимодействия признаков (interactions), то есть слагаемые вида $x_i \\cdot x_j$:\n",
    "\n",
    "$$\\hat{y}=\\langle w,x\\rangle + \\sum_{i<j}w_{ij}x_ix_j$$\n",
    "\n",
    "Помимо парных интеракций (2-order interactions), можно использовать и комбинации больших размерностей, однако на практике это редко используется.\n",
    "\n",
    "-----\n",
    "\n",
    "### Factorization Machines \n",
    "\n",
    "https://cseweb.ucsd.edu/classes/fa17/cse291-b/reading/Rendle2010FM.pdf\n",
    "\n",
    "FM - метод, который слегка модифицирует линейную модель с интеракциями:\n",
    "\n",
    "$$\\hat{y}=\\langle w,x\\rangle +\\sum_{i,j}\\langle e_i,e_j\\rangle x_ix_j$$\n",
    "\n",
    "Видим, что вместо отдельного параметра $w_{ij}$ под каждую интеракцию мы используем его оценку, посчитанную как произведение латентных представлений каждого признака $\\langle e_i,e_j\\rangle$. В результате нам не нужно считать целиком матрицу $W$, а достаточно посчитать $n$ латентных представлений признаков.\n",
    "\n",
    "Здесь и далее $e_i = e(x_i)$ - параметризованное латентное представление категориального признака $x_i$. В терминах нейронных сетей такое представление называется embedding и считается как $e(x_i)=x_iW_i$, где $W_i$ - некоторая матрица коэффициентов.\n",
    "\n",
    "-----\n",
    "\n",
    "### Matrix Factorization\n",
    "\n",
    "FM не стоит путать с широко используемым в рекомендательных системах подходом MF (Matrix Factorization), в рамках которого целевая переменная $y=f(x_{user},x_{item})$ раскладывается в произведение латентных описаний двух переменных $x_{user}$ и $x_{item}$. По аналогии с используемой выше нотацией соотвествующую модель матричного разложения можно записать так:\n",
    "\n",
    "$\\hat{y} = \\langle e_{user},e_{item}\\rangle$\n",
    "\n",
    "где $e_{user}=e(x_{user})$, $e_{item}=e(x_{item})$ - латентные сокращенные описания (они же эмбединги) признаков модели.\n",
    "\n",
    "-----\n",
    "\n",
    "### Wide-n-Deep (2016)\n",
    "\n",
    "В 2016 году Google презентовали архитектуру сети, которую назвали Wide'n'Deep\n",
    "$$\\hat{y} = \\sigma{(\\alpha_{deep}f_{dnn}(e_1 \\cdots e_n)) + \\alpha_{wide}\\sum_{i,j}w_{ij}x_ix_j})$$\n",
    "\n",
    "Ключевая идея - зависимость целевой переменной от признаков $y=f(x)$ можно декомпозировать на\n",
    "- \"wide\" часть, которая отвечает за запоминание \n",
    "- \"deep\" часть, которая отвечает за обобщение\n",
    "\n",
    "Deep часть строится как обычная сеть (DNN, Dense Nueral Network), которой на вход подаются эмбединги признаков:\n",
    "<IMG SRC=\"IMG/deep.png\" width=250>\n",
    "\n",
    "\n",
    "Wide часть - это просто набор интеракций признаков:\n",
    "<img src=\"img/wide.png\" width=250>\n",
    "\n",
    "\n",
    "В модели Wide'n'Deep эти части объединяются логистической регрессией:\n",
    "\n",
    "<img src=\"img/widendeep.png\" width=550>\n",
    "\n",
    "-----\n",
    "\n",
    "### DeepFM (2017):\n",
    "\n",
    "https://arxiv.org/abs/1703.04247\n",
    "\n",
    "Отличие архитектуры от модели wide-and-deep в том, что вместо $w_{ij}$ используется слой факторизации:\n",
    "\n",
    "$$\\hat{y} = \\sigma{(\\alpha_{deep}f_{dnn}(e_1 \\cdots e_n)) + \\alpha_{wide}\\sum_{i,j}{\\langle e_i,e_j\\rangle}x_ix_j})$$\n",
    "\n",
    "<img src=\"img/deepfm.png\" width=500>\n",
    "\n",
    "-----\n",
    "\n",
    "### xDeepFM (2018):\n",
    "\n",
    "https://arxiv.org/abs/1803.05170\n",
    "\n",
    "<img src=\"img/xdeepfm.png\" width=500>\n",
    "\n",
    "WIP\n",
    "\n",
    "-----\n",
    "\n",
    "### Neural Collaborative Filtering (NCF):\n",
    "\n",
    "https://arxiv.org/pdf/1708.05031.pdf\n",
    "\n",
    "Комбинация классического матричного разложения и глубокой модели на латентных описаниях.\n",
    "\n",
    "<img src=\"img/ncf.png\" width=500>\n",
    "\n",
    "Здесь по $x_{user}$ и по $x_{item}$ генерируется по 2 эмбединга - один (MLP vector) для глубокой сети (MLP), другой (MF vector) для матрицчного разложения (GMF layer). Затем результаты конкатенирурются и подаются на вход логистической модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepFM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "ratings = pd.read_csv('./dataset/ml-1m/ratings.dat',sep='::', header=None, engine='python', names=['uid','mid','rating','timestamp'])\n",
    "movies = pd.read_csv('./dataset/ml-1m/movies.dat',sep='::', header=None, engine='python', names=['mid','movie_name','movie_genre'])\n",
    "users = pd.read_csv('./dataset/ml-1m/users.dat',sep='::', header=None, engine='python', names=['uid','user_fea1','user_fea2','user_fea3','user_fea4'])\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, split='|',filters='', num_words=15)\n",
    "tokenizer.fit_on_texts(movies.movie_genre.values)\n",
    "seq = tokenizer.texts_to_sequences(movies.movie_genre.values)\n",
    "movies['movie_genre'] = pad_sequences(seq, maxlen=3,padding='post').tolist()\n",
    "ratings = ratings.join(movies.set_index('mid'), on = 'mid', how = 'left')\n",
    "ratings = ratings.join(users.set_index('uid'), on = 'uid', how = 'left')\n",
    "\n",
    "# ----------------------\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "def Tensor_Mean_Pooling(name = 'mean_pooling', keepdims = False):\n",
    "    return Lambda(lambda x: K.mean(x, axis = 1, keepdims=keepdims), name = name)\n",
    "\n",
    "def fm_1d(inputs, n_uid, n_mid, n_genre):\n",
    "    \n",
    "    fea3_input, uid_input, mid_input, genre_input = inputs\n",
    "    \n",
    "    # all tensors are reshape to (None, 1)\n",
    "    num_dense_1d = [Dense(1, name = 'num_dense_1d_fea4')(fea3_input)]\n",
    "    cat_sl_embed_1d = [Embedding(n_uid + 1, 1, name = 'cat_embed_1d_uid')(uid_input),\n",
    "                        Embedding(n_mid + 1, 1, name = 'cat_embed_1d_mid')(mid_input)]\n",
    "    cat_ml_embed_1d = [Embedding(n_genre + 1, 1, mask_zero=True, name = 'cat_embed_1d_genre')(genre_input)]\n",
    "\n",
    "    cat_sl_embed_1d = [Reshape((1,))(i) for i in cat_sl_embed_1d]\n",
    "    cat_ml_embed_1d = [Tensor_Mean_Pooling(name = 'embed_1d_mean')(i) for i in cat_ml_embed_1d]\n",
    "    \n",
    "    # add all tensors\n",
    "    y_fm_1d = Add(name = 'fm_1d_output')(num_dense_1d + cat_sl_embed_1d + cat_ml_embed_1d)\n",
    "    \n",
    "    return y_fm_1d\n",
    "\n",
    "def fm_2d(inputs, n_uid, n_mid, n_genre, k):\n",
    "    \n",
    "    fea3_input, uid_input, mid_input, genre_input = inputs\n",
    "    \n",
    "    num_dense_2d = [Dense(k, name = 'num_dense_2d_fea3')(fea3_input)] # shape (None, k)\n",
    "    num_dense_2d = [Reshape((1,k))(i) for i in num_dense_2d] # shape (None, 1, k)\n",
    "\n",
    "    cat_sl_embed_2d = [Embedding(n_uid + 1, k, name = 'cat_embed_2d_uid')(uid_input), \n",
    "                       Embedding(n_mid + 1, k, name = 'cat_embed_2d_mid')(mid_input)] # shape (None, 1, k)\n",
    "    \n",
    "    cat_ml_embed_2d = [Embedding(n_genre + 1, k, name = 'cat_embed_2d_genre')(genre_input)] # shape (None, 3, k)\n",
    "    cat_ml_embed_2d = [Tensor_Mean_Pooling(name = 'cat_embed_2d_genure_mean', keepdims=True)(i) for i in cat_ml_embed_2d] # shape (None, 1, k)\n",
    "\n",
    "    # concatenate all 2d embed layers => (None, ?, k)\n",
    "    embed_2d = Concatenate(axis=1, name = 'concat_embed_2d')(num_dense_2d + cat_sl_embed_2d + cat_ml_embed_2d)\n",
    "\n",
    "    # calcuate the interactions by simplication\n",
    "    # sum of (x1*x2) = sum of (0.5*[(xi)^2 - (xi^2)])\n",
    "    tensor_sum = Lambda(lambda x: K.sum(x, axis = 1), name = 'sum_of_tensors')\n",
    "    tensor_square = Lambda(lambda x: K.square(x), name = 'square_of_tensors')\n",
    "\n",
    "    sum_of_embed = tensor_sum(embed_2d)\n",
    "    square_of_embed = tensor_square(embed_2d)\n",
    "\n",
    "    square_of_sum = Multiply()([sum_of_embed, sum_of_embed])\n",
    "    sum_of_square = tensor_sum(square_of_embed)\n",
    "\n",
    "    sub = Subtract()([square_of_sum, sum_of_square])\n",
    "    sub = Lambda(lambda x: x*0.5)(sub)\n",
    "    y_fm_2d = Reshape((1,), name = 'fm_2d_output')(tensor_sum(sub))\n",
    "    \n",
    "    return y_fm_2d, embed_2d\n",
    "\n",
    "def deep_part(embed_2d, dnn_dim, dnn_dr):\n",
    "    \n",
    "    # flat embed layers from 3D to 2D tensors\n",
    "    y_dnn = Flatten(name = 'flat_embed_2d')(embed_2d)\n",
    "    for h in dnn_dim:\n",
    "        y_dnn = Dropout(dnn_dr)(y_dnn)\n",
    "        y_dnn = Dense(h, activation='relu')(y_dnn)\n",
    "    y_dnn = Dense(1, activation='relu', name = 'deep_output')(y_dnn)\n",
    "    \n",
    "    return y_dnn\n",
    "\n",
    "\n",
    "# Model Parameters\n",
    "n_uid = ratings.uid.max()\n",
    "n_mid= ratings.mid.max()\n",
    "n_genre=14\n",
    "k=20\n",
    "dnn_dim=[64,64]\n",
    "dnn_dr=0.5\n",
    "    \n",
    "# numerica features\n",
    "fea3_input = Input((1,), name = 'input_fea3')\n",
    "num_inputs = [fea3_input]\n",
    "# single level categorical features\n",
    "uid_input = Input((1,), name = 'input_uid')\n",
    "mid_input = Input((1,), name= 'input_mid')\n",
    "cat_sl_inputs = [uid_input, mid_input]\n",
    "\n",
    "# multi level categorical features (with 3 genres at most)\n",
    "genre_input = Input((3,), name = 'input_genre')\n",
    "cat_ml_inputs = [genre_input]\n",
    "\n",
    "inputs = num_inputs + cat_sl_inputs + cat_ml_inputs\n",
    "    \n",
    "# Define subnets\n",
    "y_fm_1d = fm_1d(inputs, n_uid, n_mid, n_genre)\n",
    "y_fm_2d, embed_2d = fm_2d(inputs, n_uid, n_mid, n_genre, k)\n",
    "y_dnn = deep_part(embed_2d, dnn_dim, dnn_dr)\n",
    "    \n",
    "# combinded deep and fm parts\n",
    "y = Concatenate()([y_fm_1d, y_fm_2d, y_dnn])\n",
    "y = Dense(1, name = 'deepfm_output')(y)\n",
    "    \n",
    "fm_model_1d = Model(inputs, y_fm_1d)\n",
    "fm_model_2d = Model(inputs, y_fm_2d)\n",
    "deep_model = Model(inputs, y_dnn)\n",
    "deep_fm_model = Model(inputs, y)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "\n",
    "\n",
    "# Format Dataset\n",
    "def df2xy(ratings):\n",
    "    x = [ratings.user_fea3.values, \n",
    "         ratings.uid.values, \n",
    "         ratings.mid.values, \n",
    "         np.concatenate(ratings.movie_genre.values).reshape(-1,3)]\n",
    "    y = ratings.rating.values\n",
    "    return x,y\n",
    "\n",
    "in_train_flag = np.random.random(len(ratings)) <= 0.9\n",
    "train_data = ratings.loc[in_train_flag,]\n",
    "valid_data = ratings.loc[~in_train_flag,]\n",
    "train_x, train_y = df2xy(train_data)\n",
    "valid_x, valid_y = df2xy(valid_data)\n",
    "\n",
    "# train  model\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "deep_fm_model.compile(loss = 'MSE', optimizer='adam')\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model_ckp = ModelCheckpoint(filepath='./model/deepfm_weights.h5', \n",
    "                            monitor='val_loss',\n",
    "                            save_weights_only=True, \n",
    "                            save_best_only=True)\n",
    "callbacks = [model_ckp,early_stop]\n",
    "train_history = deep_fm_model.fit(train_x, train_y, \n",
    "                                  epochs=30, batch_size=2048, \n",
    "                                  validation_split=0.1, \n",
    "                                  callbacks = callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Collaborative Filtering\n",
    "\n",
    "Попробуем релаизовать подход NCF с помощью библиотеки Keras. В качестве обучающей выборки используем датасет MovieLens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = pd.read_csv(\"/Users/nipun/Downloads/ml-100k/u.data\",sep='\\t',names=\"user_id,item_id,rating,timestamp\".split(\",\"))\n",
    "\n",
    "dataset.user_id = dataset.user_id.astype('category').cat.codes.values\n",
    "dataset.item_id = dataset.item_id.astype('category').cat.codes.values\n",
    "\n",
    "# Train/Test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(dataset, test_size=0.2)\n",
    "\n",
    "y_true = test.rating\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "\n",
    "# Model Parameters\n",
    "n_latent_factors_user = 8\n",
    "n_latent_factors_movie = 10\n",
    "n_latent_factors_mf = 3\n",
    "n_users, n_movies = len(dataset.user_id.unique()), len(dataset.item_id.unique())\n",
    "\n",
    "movie_input = keras.layers.Input(shape=[1],name='Item')\n",
    "movie_embedding_mlp = keras.layers.Embedding(n_movies + 1, n_latent_factors_movie, name='Movie-Embedding-MLP')(movie_input)\n",
    "movie_vec_mlp = keras.layers.Flatten(name='FlattenMovies-MLP')(movie_embedding_mlp)\n",
    "movie_vec_mlp = keras.layers.Dropout(0.2)(movie_vec_mlp)\n",
    "\n",
    "movie_embedding_mf = keras.layers.Embedding(n_movies + 1, n_latent_factors_mf, name='Movie-Embedding-MF')(movie_input)\n",
    "movie_vec_mf = keras.layers.Flatten(name='FlattenMovies-MF')(movie_embedding_mf)\n",
    "movie_vec_mf = keras.layers.Dropout(0.2)(movie_vec_mf)\n",
    "\n",
    "user_input = keras.layers.Input(shape=[1],name='User')\n",
    "user_vec_mlp = keras.layers.Flatten(name='FlattenUsers-MLP')(keras.layers.Embedding(n_users + 1, n_latent_factors_user,name='User-Embedding-MLP')(user_input))\n",
    "user_vec_mlp = keras.layers.Dropout(0.2)(user_vec_mlp)\n",
    "\n",
    "user_vec_mf = keras.layers.Flatten(name='FlattenUsers-MF')(keras.layers.Embedding(n_users + 1, n_latent_factors_mf,name='User-Embedding-MF')(user_input))\n",
    "user_vec_mf = keras.layers.Dropout(0.2)(user_vec_mf)\n",
    "\n",
    "\n",
    "concat = keras.layers.merge([movie_vec_mlp, user_vec_mlp], mode='concat',name='Concat')\n",
    "concat_dropout = keras.layers.Dropout(0.2)(concat)\n",
    "dense = keras.layers.Dense(200,name='FullyConnected')(concat_dropout)\n",
    "dense_batch = keras.layers.BatchNormalization(name='Batch')(dense)\n",
    "dropout_1 = keras.layers.Dropout(0.2,name='Dropout-1')(dense_batch)\n",
    "dense_2 = keras.layers.Dense(100,name='FullyConnected-1')(dropout_1)\n",
    "dense_batch_2 = keras.layers.BatchNormalization(name='Batch-2')(dense_2)\n",
    "\n",
    "\n",
    "dropout_2 = keras.layers.Dropout(0.2,name='Dropout-2')(dense_batch_2)\n",
    "dense_3 = keras.layers.Dense(50,name='FullyConnected-2')(dropout_2)\n",
    "dense_4 = keras.layers.Dense(20,name='FullyConnected-3', activation='relu')(dense_3)\n",
    "\n",
    "pred_mf = keras.layers.merge([movie_vec_mf, user_vec_mf], mode='dot',name='Dot')\n",
    "\n",
    "\n",
    "pred_mlp = keras.layers.Dense(1, activation='relu',name='Activation')(dense_4)\n",
    "\n",
    "combine_mlp_mf = keras.layers.merge([pred_mf, pred_mlp], mode='concat',name='Concat-MF-MLP')\n",
    "result_combine = keras.layers.Dense(100,name='Combine-MF-MLP')(combine_mlp_mf)\n",
    "deep_combine = keras.layers.Dense(100,name='FullyConnected-4')(result_combine)\n",
    "\n",
    "\n",
    "result = keras.layers.Dense(1,name='Prediction')(deep_combine)\n",
    "\n",
    "\n",
    "model = keras.Model([user_input, movie_input], result)\n",
    "opt = keras.optimizers.Adam(lr =0.01)\n",
    "model.compile(optimizer='adam',loss= 'mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель и посмотрим, какое качество она показывает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([train.user_id, train.item_id], train.rating, epochs=25, verbose=0, validation_split=0.1)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "y_hat_2 = np.round(model.predict([test.user_id, test.item_id]),0)\n",
    "print(mean_absolute_error(y_true, y_hat_2))\n",
    "\n",
    "print(mean_absolute_error(y_true, model.predict([test.user_id, test.item_id])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
