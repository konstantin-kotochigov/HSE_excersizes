{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">**Занятие 1 (NLP)**</span>\n",
    "- <span style=\"color:gray\">Занятие 2 (NLP)</span>\n",
    "- <span style=\"color:gray\">Занятие 3 (NLP)</span>\n",
    "- <span style=\"color:gray\">Занятие 4 (Recommender Systems)</span>\n",
    "- <span style=\"color:gray\">Занятие 5 (Recommender Systems)</span>\n",
    "- <span style=\"color:gray\">Занятие 6 (CRISP-DM)</span>\n",
    "- <span style=\"color:gray\">Занятие 7 (to be anounced)</span>\n",
    "- <span style=\"color:gray\">Занятие 8 (to be anounced)</span>\n",
    "- <span style=\"color:gray\">Занятие 9 (Class Hours)</span>\n",
    "- <span style=\"color:gray\">Занятие 10 (Class Hours)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Введение\n",
    "\n",
    "В задачах машинного обучения текстовые данные могут использоваться точно так же, как и обычные числовые признаки. Единственное требование - нужно научиться как-то их представить в виде числового вектора:\n",
    "\n",
    "$$\\text{This is some text data} \\rightarrow (1,2,3)$$\n",
    "\n",
    "Процесс представления текста в виде числовых векторов называется векторизацией и на этом занятии мы посмотрим, какие существуют типичные подходы для векторизации текстов.\n",
    "\n",
    "#### Требования\n",
    "- установленная библиотека scikit-learn\n",
    "\n",
    "    pip install scikit-learn\n",
    "    \n",
    "    \n",
    "- установленная библиотека nltk\n",
    "\n",
    "    pip install nltk\n",
    "\n",
    "Введем несколько основных понятий.\n",
    "\n",
    "#### Глоссарий\n",
    "- <span style=\"color:blue\">Document</span> - единица текста, соотвествующая одному наблюдению. Например, это может быть статья, отзыв, предложение.\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Corpus (корпус)</span> - множество всех документов, которыми мы располагаем.\n",
    "\n",
    "    Например, существуют корпусы текстов Википедии, корпус текстов русского языка, корпус текстов художественной литературы и т.д.\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Term</span> (или <span style=\"color:blue\">Token</span>) - наименьшие единицы, на которые разбивается текст. \n",
    "\n",
    "     $$\\text{Document} = \\text{\"This is some text data\"}$$\n",
    "     \n",
    "     $$\\text{Tokens} = \\text{('This','is','some','text','data')}$$\n",
    "\n",
    "    Чаще всего токеном/термом является слово, поэтому по умолчанию будем считать term = слово. Однако термами могут быть, например, и буквы, и n-граммы, все зависит от решаемой задачи.\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">N-gram/shingle</span> - последовательность из нескольких подряд идущих токенов.\n",
    "\n",
    "$$\\text{2-grams} = \\text{('This is') ('is some') ('some text') ('text data'))}$$\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Document-Term matrix</span>  - матрица, в которой по строкам отложены документы, а по столбцам термы, значением же является некая мера ассоциированности терма с документом - часто это просто частота данного терма в документе.\n",
    "\n",
    "<img src=\"img/document_term.png\" width=500>\n",
    "\n",
    "    Также иногда работают с ее транспонриованным вариантом Term-Document matrix.\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Vocabulary</span> - набор всех термов из корпуса и их соотвествующие номера измерений в векторном пространстве\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Vector-Space Model</span> - способ представления терма в виде многомерного вектора\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Term Frequency</span> - частота встречаемости терма в данном документе\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Document Frequency</span> - в каком проценте документов корпуса встречается данный терм\n",
    "\n",
    "\n",
    "- <span style=\"color:blue\">Inverse Docuemnt Frequency</span> - 1/DF, коэффициент \"уникальности\" терма, чем он выше, тем реже терм встречается в текстах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим небольшой корпус из 3 документов. В Python это будет просто список из 3 строк."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To be, or not to be: that is the question:',\n",
       " 'to be be to',\n",
       " 'I went to party yesterday']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['To be, or not to be: that is the question:','to be be to', 'I went to party yesterday']\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке scikit-learn существует 3 вида векторайзеров:\n",
    "- CountVectorizer (<a link=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">doc</a>)\n",
    "- HashingVectorizer (<a link=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html\">doc</a>)\n",
    "- TfidfVectorizer (<a link=\"https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\">doc</a>)\n",
    "\n",
    "На вход подается список текстов, на выходе имеем Document-Term матрицу, описывающую данные тексты\n",
    "\n",
    "Ниже рассмотрим каждый из них и проговорим отличия."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для начала попробуем **CountVectorizer**. При создании объекта пока используем настройки по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортируем нужный нам класс. Все векторайзеры находятся в sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Далее нужно создать экземпляр класса, который будем использовать\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напомню, что в Sklearn все транформации содержат 2 стандартных метода\n",
    "- fit(), который \"настраивает\" трансофрмацию на переданных в качестве аргумента данных\n",
    "- transform(), который применяет трансформацию к переданным ему в качестве аргумента данным\n",
    "\n",
    "Соответственно класс CountVectorizer также сождержит эти два метода\n",
    "- fit() - анализирует передаваемый ему в качестве аргумента корпус текстов и составляет словарь термов\n",
    "- transform() - преобразует текст в векторное представление в соответствии с построенным на этапе fit() словарем\n",
    "\n",
    "Поскольку очень часто эти действия выполняются одно за другим, есть еще метод\n",
    "- fit_transform()\n",
    "\n",
    "который выполняет их последовательно за один вызыв метода (построение словаря на корпусе текстов и преобразование этого корпуса в векторный формат)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сначала \"обучим\" наш векторайзер (vect) на корпусе text\n",
    "vect.fit(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмореть содержание построенного словаря мы можем в атрибуте vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'to': 8,\n",
       " 'be': 0,\n",
       " 'or': 3,\n",
       " 'not': 2,\n",
       " 'that': 6,\n",
       " 'is': 1,\n",
       " 'the': 7,\n",
       " 'question': 5,\n",
       " 'went': 9,\n",
       " 'party': 4,\n",
       " 'yesterday': 10}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что каждому уникальному терму назначен свой порядковый номер (0,1,2...) \n",
    "\n",
    "Порядковые номера назначаются по мере того, как новые слова встречаются в документе, однако порядок нумерации здесь не имеет никакого значения. Подобное представление текста называется bag-of-words. Согласно подходу BOW фразы 'я тебя люблю' и 'люблю я тебя' имеют одно и то же векторное представление."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Теперь применим \"обученный\" векторайзер к нашему тексту\n",
    "text_vectorized = vect.transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе мы получаем Document-Term матрицу (то есть набор векторизированных текстов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11)\n"
     ]
    }
   ],
   "source": [
    "print(text_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность этой матрицы (3,11). \n",
    "\n",
    "Действительно, у нас 3 документа и 11 уникальных слов. \n",
    "\n",
    "_(не 12, так как местоимение \"I\" у нас по умолчанию отфильтровывалось как незначимое. Позднее увидим, как это можно отрегулировать)_\n",
    "\n",
    "Теперь посмотрим, что из себя представляет эта матрица"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(text_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что тип возвращаемого результата - [sparse-матрица](https://en.wikipedia.org/wiki/Sparse_matrix).\n",
    "\n",
    "Разреженные матрицы позволяют эффективно хранить большие объемы данных, когда у матрицы очень большая размерность (например 1,000,000 на 1,000,000), но при этом лишь небольшое количество элементов имеет ненулевые значения. Поэтому такой тип данных является стандартом при работе с матрицами в scikit-learn. \n",
    "\n",
    "Если интересно, за счет чего оптимизируется хранение и какие есть виды sparse-матриц, рекомендую почитать например в [документации](https://docs.scipy.org/doc/scipy/reference/sparse.html) scipy.\n",
    "\n",
    "Единственный минус разреженных матриц - мы не можем вывести её на экран как обычную матрицу (она для этого не предназначена). Однако поскольку в нашем модельном пример данных немного, мы можем явно преобразовать её в стандартную dense-матрицу и уже вывести на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2, 1, 1, 1, 0, 1, 1, 1, 2, 0, 0],\n",
       "        [2, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что означают эти цифры?\n",
    "\n",
    "<span style=\"color:blue\">Count</span>Vectorizer выводит в Document-Term матрице <span style=\"color:blue\">число</span> вхождений кажого терма. \n",
    "\n",
    "Для удобства давайте выведем эту матрицу вместе с подписанными термами и документами. Для этого сделаем несколько манипуляций как показано ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>be</th>\n",
       "      <th>is</th>\n",
       "      <th>not</th>\n",
       "      <th>or</th>\n",
       "      <th>party</th>\n",
       "      <th>question</th>\n",
       "      <th>that</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>went</th>\n",
       "      <th>yesterday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>To be, or not to be: that is the question:</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to be be to</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I went to party yesterday</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            be  is  not  or  party  question  \\\n",
       "To be, or not to be: that is the question:   2   1    1   1      0         1   \n",
       "to be be to                                  2   0    0   0      0         0   \n",
       "I went to party yesterday                    0   0    0   0      1         0   \n",
       "\n",
       "                                            that  the  to  went  yesterday  \n",
       "To be, or not to be: that is the question:     1    1   2     0          0  \n",
       "to be be to                                    0    0   2     0          0  \n",
       "I went to party yesterday                      0    0   1     1          1  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Сконвертируем матрицу в удобный формат Pandas DataFrame\n",
    "df = pd.DataFrame(text_vectorized.todense())\n",
    "\n",
    "# Инвертируем наш словарь, чтобы он по номеру возвращал терм\n",
    "colnames_dict = {v:k for k,v in vect.vocabulary_.items()}\n",
    "\n",
    "# Переименуем названия колонок в термы\n",
    "df.columns = [colnames_dict[x] for x in df.columns]\n",
    "\n",
    "df.index = text\n",
    "\n",
    "# Выведем датафрейм\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, терм под номером 0 (\"be\") встречается 2 раза в первом и втором текстах, а терм под номером 1 (\"is\") встречается только в первом тексте и один раз.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При создании класса CountVectorizer мы можем задавать несколько параметров, тем самым настраивая процесс векторизации. Расммотрим подробнее.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-грамм - последовательность из n подряд идущих слов или символов\n",
    "\n",
    "$$\\text{1-grams} = \\text{('This') ('is') ('some') ('text') ('data')}$$\n",
    "\n",
    "$$\\text{2-grams} = \\text{('This is') ('is some') ('some text') ('text data'))}$$\n",
    "\n",
    "$$\\text{3-grams} = \\text{('This is some') ('is some text') ('some text data')}$$\n",
    "\n",
    "Чтобы использовать n-граммы, устанавливаем параметр ngram_range - он задает интервал длин n-граммов, которые будут рассчитаны. \n",
    "\n",
    "Например, \n",
    "- (2,2) говорит использовать 2-граммы (биграммы)\n",
    "- (1,2) будет использовать 1-граммы (униграммы) и 2-граммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 13)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'to be': 10,\n",
       " 'be or': 1,\n",
       " 'or not': 6,\n",
       " 'not to': 5,\n",
       " 'be that': 2,\n",
       " 'that is': 8,\n",
       " 'is the': 4,\n",
       " 'the question': 9,\n",
       " 'be be': 0,\n",
       " 'be to': 3,\n",
       " 'went to': 12,\n",
       " 'to party': 11,\n",
       " 'party yesterday': 7}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что словарь выглядит уже по-другому - он состоит из всех уникальных последовательных пар слов, которые встречаются в тексте. \n",
    "\n",
    "Преобразованный с помощью этого словря текст представляет собой 3 вектора в пространстве большей размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 2, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике часто комбинируют n-граммы разной размерности. При построении моделей это позволяет находить паттерны не только в словах, но и в словосочетаниях и устойчивых выражениях. \n",
    "\n",
    "Ниже пример, как рассчитать 1-граммы и 2-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию тексты переводятся в нижний регистр. Иногда регистр важен, в этом случае, его можно отключить опцией lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(lowercase=False)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда с текстами работают не на уровне слов или n-грамов, а на уровне символов (это встречается, например, в задачах генерации). Мы можем задать, на что будем разбивать наши документы, параметром analyzer - для символов это analyzer='char', а для слов (по умолчанию) это analyzer='word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(analyzer='char')\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можем явно задать фильтрацию неинформативных стоп-слов. Они передаются списком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=['to','a','then'])\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что отфильтровалось слово to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важная функция - мы можем отфильтровать термы со слишком большой или слишком малой частотой использования в корпусе (document frequency).\n",
    "\n",
    "Эти пороги задает пара параметров: \n",
    "- min_df\n",
    "- max_df\n",
    "\n",
    "Большой DF говорит о том, что слово скорее всего малоинформативно - оно характерно для языка в целом, но не характерно для каких-либо классов документов (а это нехорошо, если мы захотим решать задачу классификации на этих данных). С большим DF обычно предлоги, союзы и общеупотребительные слова и выражения.\n",
    "\n",
    "Термы с малым DF слабо влияют на целевую переменную, занимая при этом место. Их можно спокойно отфильтровать.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=0.01, max_df=0.5)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы не хотим хранить и каждый раз обновлять словарь, можем воспользоваться алгоритмом хэширования слов. Аналогом класса CountVectorizer с реализованным hashing trick является класс **HashingVectorizer**. Определение индекса слова в векторном представлении документа просиходит динамически с помощью хэш-функции.\n",
    "\n",
    "Обратите внимание, что по умолчанию, HasingVectorizer нормирует получаемый вектор документа. Для иллюстрации пока отключим нормировку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vect = HashingVectorizer()\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на размерность данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не видно. Чтобы найти индексы наших 8 термов можно сделать так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.nonzero(text_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексы знаем, теперь можно посмотреть сами значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized[numpy.nonzero(text_vectorized)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один часто используемый векторайзер это класс **TfidfVectorizer**. Он является аналогом CountVectorizer, но по умолчанию также включает TF-IDF преобразование частот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[2.57536414, 1.69314718, 1.69314718, 1.69314718, 0.        ,\n",
       "         1.69314718, 1.69314718, 1.69314718, 2.        , 0.        ,\n",
       "         0.        ],\n",
       "        [2.57536414, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 2.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 1.69314718,\n",
       "         0.        , 0.        , 0.        , 1.        , 1.69314718,\n",
       "         1.69314718]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(norm=False)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Параметр **norm**:\n",
    "- None - нет никакой дополнительной нормировки\n",
    "- l1 - после того как получили векторное представление текста, дополнительно нормируем его, чтобы сумма равнялась 1\n",
    "- l2 - после того как получили векторное представление текста, дополнительно нормируем его, чтобы сумма квадратов равнялась 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 11)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.48815614, 0.32093333, 0.32093333, 0.32093333, 0.        ,\n",
       "         0.32093333, 0.32093333, 0.32093333, 0.37909679, 0.        ,\n",
       "         0.        ],\n",
       "        [0.78980693, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.61335554, 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        , 0.54645401,\n",
       "         0.        , 0.        , 0.        , 0.32274454, 0.54645401,\n",
       "         0.54645401]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(norm='l2')\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типичная последовательность шагов при препроцессинге текста выглядит так: \n",
    "- нормализация (удаление неинформативных символов, цифр)\n",
    "- токенизация (разделение текста на смысловые единицы)\n",
    "- лемматизация (приведение слова к нормальной форме, именительный падеж, единственное число)\n",
    "- стемминг (выделение основы слова)\n",
    "- выделение частей речи\n",
    "- фильтрация стоп-слов (соединительных слов, союзов и предлогов, специфичных для доменной области неинформативных слов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кастомные обработчики"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если требуется реализация более сложной логики предобработки текста, мы можем кастомизировать описанные выше векторайзеры и прописать её самостоятельно в специальном классе MyTokenizer.\n",
    "\n",
    "В этом классе должен быть определен метод **\\__call\\__**, который выполняется для каждого документа и куда в качестве аргумента передается текст для обработки (doc). \n",
    "\n",
    "Ниже пример такого класса (для запуска нужен установленный nltk). Он:\n",
    "- разделяет текст на слова\n",
    "- по каждому слову определяется его части речи\n",
    "- убираем из слов предлоги и неразмеченные слова\n",
    "- делаем лемматизацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = Mystem()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        pos = pos_tag(tokens, lang='rus')\n",
    "        pos = [x[0] for x in pos if (x[1] not in [\"NONLEX\",\"CONJ\"]) or (x[0] == 'eos')]\n",
    "        lemmatized_tokens = [self.wnl.lemmatize(t)[0] for t in pos]\n",
    "        return (lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда создали такой класс, передаем его в качестве аргумента при создании векторайзера (параметр tokenizer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(\n",
    "    tokenizer = MyTokenizer(), \n",
    "    min_df=0.001,\n",
    "    max_df=0.75, \n",
    "    stop_words = russian_stop_words, \n",
    "    lowercase=True, \n",
    "    ngram_range=(1,2))\n",
    "\n",
    "text_vectorized = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "Далее несколько примеров преобразований из библиотеки NLTK\n",
    "\n",
    "Разделение на токены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "doc = word_tokenize('Я пришел в столовую и съел волшебных котлет')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Only English\n",
    "lemmatizer.lemmatize(\"fugitives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определение части речи:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "tags = pos_tag(doc, lang='rus')\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что тэгер каждое слово текста относит к определенной части речи (V - глагол, CONJ - союз, PR - предлог и так далее). \n",
    "\n",
    "В основе тэгера лежит самообучаемая предиктивная модель. Она не использует словари, поэтому вполне работает и с выдуманными языком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pos_tag(word_tokenize('кареокий шляндр высвестил поорекомандно кнодлик'), lang='rus')\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кейс: классификатор на основе текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем решить более приближенную к реальности задачу. \n",
    "\n",
    "В sklearn есть отличный пакет datasets с реальными наборами данных. \n",
    "\n",
    "Возьмем оттуда датасет 20newsgroups - корпус текстов из новостных сообществ, разделенных по 20 темам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset = train, выгружаем часть данных под обучение\n",
    "# shuffle=True - считываем записи в случайном порядке, некоторые модели хуже обучаются на датасетах с жестко заданным порядком\n",
    "text_dataset = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим что внутри"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри 5 переменных:\n",
    "- **data:** собственно сами тексты, X\n",
    "- **target:** номер класса [0:19], y\n",
    "- **target_names:** название класса\n",
    "- **DESCR:** текстовое описание выборки\n",
    "- **filenames:** имена файлов, куда он скачал выборку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Повторяем процесс векторизации, который делали выше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: есть ещё класс TfidfTransformer, который делает только взвешивание, но ничего не векторизует. \n",
    "\n",
    "Соотвественно применение TfidfVectorizer() = последовательно примененные CountVectorizer() и TfidfTransformer(). \n",
    "\n",
    "Давайте попробуем расписать в 2 шага:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "# Считаем вхождения термов в каждый документ\n",
    "X_train_counts = count_vect.fit_transform(text_dataset.data)\n",
    "\n",
    "# Взвешиваем полученное векторное представление с помощью TF-IDF преобразования\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучающая выборка готова (X_train_tfidf), теперь нужен классификатор.\n",
    "\n",
    "Для классификации попробуем использовать [Multinomial NaiveBayes](https://scikit-learn.org/stable/modules/naive_bayes.html) классификатор. Это простейший классификатор среди тех, которые обычно применяют для классификации текстовых данных. Логика его работы основана на подчсете частотности появления слов в каждом классе.\n",
    "\n",
    "В метод fit() передаем обучающую выборку: матрицу предкиторов (X) и целевую переменную (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, text_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор обучен, теперь проверим, как он работает на 2 тестовых примерах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15  7]\n"
     ]
    }
   ],
   "source": [
    "# Создаем пару тестовых примеров\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "\n",
    "# Векторизуем их\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "# Применяем обученный классификатор\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получили список с номерами предсказанных классов по каждому тексту. Вместо номеров выведем их текстовые названия:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => rec.autos\n"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, text_dataset.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что классификатор рабртает корректно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательности трансформаций удобно объединять в пайплайны. Замыкает пайплайн всегда классификатор.\n",
    "\n",
    "Для каждой трансформации прописывается её имя (потом по имени можно обращаться к параметрам водящих в пайплайн трансоформаций)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения Sklearn pipeline - это обычный классификатор и у него тоже есть методы fit() и predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(text_dataset.data, text_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такой вызов равносилен последовательному вызову \n",
    "- CountVectorizer.fit_transform(), \n",
    "- TfidfTransformer.fit_transform() и \n",
    "- MultinomialNB.fit() \n",
    "\n",
    "на выборке text_dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проверим, насколько точен наш полученный классификатор.\n",
    "\n",
    "Для этого:\n",
    "- загрузим тестовую часть данных \n",
    "- применим к ней нашу обученную модель\n",
    "- посчитаем, в каком проценте тестовых кейсов прогноз верен"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7738980350504514"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Test Dataset\n",
    "text_dataset_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "docs_test = text_dataset_test.data\n",
    "\n",
    "# Apply Model to new data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "\n",
    "# Assess Quality\n",
    "sum(predicted == text_dataset_test.target) / len(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем использовать другой алгоритм классификации - SVM, который также является стандартным алгоритмом (эпохи доглубокого обучения) для работы с текстами.\n",
    "\n",
    "Логика полностью повторяется, только заменяем классификатор на другую модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "text_clf.fit(text_dataset.data, text_dataset.target)  \n",
    "predicted = text_clf.predict(docs_test)\n",
    "numpy.mean(predicted == text_dataset_test.target)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как определеить, какие значения параметров лучше использовать для классификатора? \n",
    "\n",
    "Для этого существует [Grid Search](https://en.wikipedia.org/wiki/Hyperparameter_optimization) - способ запуска процесса обучения, при котором в цикле перебираются возможные наборы параметров, а затем находится оптимальный.\n",
    "\n",
    "Для начала необходимо создать сетку параметров, на которой мы будем искать оптимум. По каждому параметру мы просто списком перечисляем значения, которые мы хотим, чтобы он принимал.\n",
    "\n",
    "Можно задавать параметры отдельных трансформаций. Для этого достаточно указать имя шага (как в пайплайне) и через два подчеркивания имя параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для автоматизации процесса обучения на сетке параметров в sklearn существует класс [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Его параметры:\n",
    "- Классификатор или пайплайн, который нужно запускать \n",
    "- Сетка параметров\n",
    "- Вид кросс-валидации. Например, если cv=5, то будет использована 5-кратная k-fold валидация. Кросс-валидация нужна для получения более устойчивых метрик качества по каждому набору парметров. Значение метрики усредняется между этих 5 запусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cvGrid = GridSearchCV(text_clf, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданная выше сетка предполагает 2^3 = 8 различных комбинаций параметров.\n",
    "\n",
    "CV = 5 запускает процесс обучения 5 раз\n",
    "\n",
    "Итого, требуется 5 * 8 = 40 запусков. Расчет может занять продолжительное время. Попробуем запуститься на небольшом помножестве выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.fit(text_dataset.data, text_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как посмотреть точность наилучшей модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так же есть словарик cv results_, который содержит всю информацию об обучении. Для нас наиболее важен массив mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.cv_results_['mean_test_score']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
