{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Термины \"векторизация\" или \"вложение\" (**embedding**) означают представление объекта в виде вектора в неком пространстве. Применяются они, как правило, к нечисловым данным (таким как тексты, графы, последовательности), поскольку числовые данные и так без проблем представляются в виде вектора.\n",
    "\n",
    "Если мы хотим использовать нечисловые объекты в ML-моделях, такое представление необходимо.\n",
    "\n",
    "Самое простое векторное представление объекта это **one-hot encoding** (оно же **dummy encoding**). Работает так - мы перечисляем все возможные типы объектов и ставим 1 напротив нашего типа. Например, если мы последовательно кодируем три символа a,b и c, то one-hot encoding для каждого выглядит так:\n",
    "a=>[1,0,0], b=>[0,1,0], c=>[0,0,1]\n",
    "\n",
    "One-hot encoding иногда используют в задачах ML, но для текстов или других данных большой размерности оно крайне неээфективно => нужен другой способ кодирования."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec** - алгоритм векторизации слова, при котором часто встречающиеся в одном контексте слова кодируются близкими векторами. Под контекстом понимается \n",
    "\n",
    "Разработан в 2013 году Томасом Миколовым. Оригинал статьи: https://arxiv.org/abs/1301.3781\n",
    "\n",
    "Если получится закодировать слова двумерными векторами, то мы сможем делать красивые визуализации множества слов\n",
    "\n",
    "<img src=\"img/wordmap.png\" width=500>\n",
    "\n",
    "Слово, которое нужно векторизовать и его контекст - соседние слова слева и справа. Слово зависит от контекста и наоборот контекст зависит от слова.\n",
    "\n",
    "\n",
    "\n",
    "Есть две формулировки задачи Word2Vec:\n",
    "\n",
    "- **Skip-Gram**\n",
    "\n",
    "  подбираем модель так, чтобы для каждого слова наблюдаемый контекст (скажем 5 соседних слов) был наиболее вероятен\n",
    "\n",
    "\n",
    "- **CBOW** (continuous Bag-Of-Words)\n",
    "\n",
    "  подбираем модель так, чтобы для каждого контекста наблюдаемое слово было наиболее вероятным\n",
    "\n",
    "С точки зрения архитектуры модели всё идентично. Отличаюся способы обучения.\n",
    "                  \n",
    "К сожалению в исходной статье алгоритм описан довольно абстрактно и не предлагает ссылок на конекртные реализации. Приходится их додумывать.\n",
    "\n",
    "Можно с одной матрицей U обучать сеть, тогда на вход идёт пара (слово, другое слово). Как вот тут для CBOW:\n",
    "Здесь мы просто притягиваем P(word, context_word) к нашим данным. \n",
    "\n",
    "Но при таком подходе на выходе только одна вероятность, а если умножать на матрицу V, можем сразу получить распределение веротяностей за один проход по сети.\n",
    "\n",
    "Схема получения Word2Vec эмбедингов:\n",
    "Обучаем модель\n",
    "Отбрасываем правую часть нейронной сети (Head)\n",
    "Выход Encoder слоя - это и есть эмбединги слова, которое подается на вход. \n",
    "\n",
    "Математически задача решается как задача максимизации правдоподобия:\n",
    "\n",
    "- $J()=w_cP(w|c) \\rightarrow max$\n",
    "\n",
    "- $J()=w_cP(c|w) \\rightarrow max$\n",
    "\n",
    "\n",
    "Обратите внимание, что в рамках модели Word2Vec вложение слова не зависит от соседних слов в кодируемом предложении - оно всгеда одинаково. Это минус, когда мы имеем дело с омонимами (одно написание, но разные смыслы).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Как обучается Word2Vec?\n",
    "\n",
    "\n",
    "\n",
    "Подбираются такие вектора wi и wj, что веротяность текста максимизируется. В итоге высокая вероятность пары слов наблюдается там, где слова из одного контекста часто встречаются вместе.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Практика\n",
    "### 2.1 Реализация в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать нейронную сеть для построения эмбедингов алгоритмом Word2Vec. Будем использовать API Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, merge, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import urllib\n",
    "import collections\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужен большой корпус текстов.\n",
    "\n",
    "У чувака есть предобработанный датасет с текстами из википедии (без знаков препинания). Весит около 30MB, можно скачать по прямой ссылке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'http://mattmahoney.net/dc/text8.zip'\n",
    "!unzip -o text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаться будем на 17 млн слов. \n",
    "\n",
    "Небольшой минус данного датасета - текст не делится на предложения и в конец статьи может попасть в начало другой при определении контекстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"text8\",\"r\")\n",
    "words = file.read().split()\n",
    "file.close()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем частоту слов. В питоне для этого есть специальный класс collections.Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "word_freq = collections.Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ожидаемо наиболее частотные слова - артикли и числительные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополним список специальным словом 'UNK' для обозначения более редких слов, чем с частотой n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = 10000\n",
    "word_freq_list = [['UNK', -1]]\n",
    "word_freq_list.extend(word_freq.most_common(n_words - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставим в соотвествие каждому слову свой порядковый номер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "for word, _ in word_freq_list:\n",
    "        dictionary[word] = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируем текст в последовательность индексов слов. Заодно посчитаем частоту группы 'UKN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "unk_count = 0\n",
    "for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "count[0][1] = unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наш текст выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом когда будем анализировать результаты, нам наверняка понадобится обратный словарь (из кода слова в символьное представление)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "reversed_dictionary[1234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим основные параметры обучения.\n",
    "- число слов в словаре (vocab_size), мы ранее сами задали ограничение в 10000 слов, поэтому пишем 10000\n",
    "- размер контекста слова (window_size), сколько сосдених слов будем брать с каждой стороны в качестве контекста слова\n",
    "- какую хотим получить размерность эмбединга (vector_dim)\n",
    "- число эпох для обучения нейросети (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим частотное распределние слов для Negative Sampling. Для этого в Keras есть специальный метод sequence.make_sampling_table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "sampling_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в Keras есть метод для negative-sampling. Он генерирует положительные примеры (слово и слово из контекста) и отрицательные примеры (слово и произвольное слово). Слова выбираются исходя из частот полученных выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядят сэмплы примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(couples[0:10], labels[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши пары слов на 2 отдельных списка. В питоне для этого есть функция zip(*).\n",
    "\n",
    "Мы нагенерировали 30 млн пар слов, поэтому обработка займет какое-то время (может 5-10 минут)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_target, word_context = zip(*couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем собирать нейронную сеть. Схема ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/word2vec_keras.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 2 входа сети - один для анализируемого слова (target) и один для слова из его контекста (context). Оба - скаляры (код слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее каждый вход подключаем к эмбедингу (он один и тот же для обоих входов). \n",
    "\n",
    "В Keras слой Embedding - это просто словарик с набором описаний, элементы которого сеть считает весами. По мере обучения сети веса выстраиваются так, чтобы достигался минимальный loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемножаем эмбединги обоих входов. Для скалярного произведения тензоров в Keras сущесвтует слой Dot.\n",
    "\n",
    "При создании слоя Dot ставим normalize = True. Это означает, что скалряное произведение становится косинусным антирасстоянием (cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = Dot(axes=0, normalize=True)([target, context])\n",
    "dot_product = Reshape((1,))(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас softmax бинарная классификация (мы оцениваем вероятность события \"слово 1 входит в контекст слова 2\").\n",
    "\n",
    "Поэтому выход определяем как Dense слой со скалярным выходом и сигмоидной активацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем и компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем вариант модели с другой функцией билзости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пара функций для визуального тестирования результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reversed_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reversed_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "    \n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждую эпоху дообучаем сеть на одном примере. Через каждые 10000 примеров для 16 случайно выбранных слов выводим его ближайшие слова.\n",
    "\n",
    "**Note1:** на ноутбуке сеть может обучаться очень долго. Для целей презентации подхода этого достаточно. Для достижения реального результата очень желательно запускать обучение на мощных серверах либо GPU.\n",
    "\n",
    "**Note2:** Чтобы сеть обучалась эффективнее, надо все же объединять сэмплы в батчи большего размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения модели можем выгрузить эмбединги слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем мапинг из слова в эмбединг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding = {w:embeddings[idx] for w,idx in dictionary.items()}\n",
    "word2embedding['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec в бибилотеке Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-c493007d98c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommon_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим и обучим модель Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(common_texts, \n",
    "    size=100, \n",
    "    window=5, \n",
    "    min_count=1, \n",
    "    workers=4)\n",
    "\n",
    "model.fit(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.vw['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обученные веса можно сохранить на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.wv['computer']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
