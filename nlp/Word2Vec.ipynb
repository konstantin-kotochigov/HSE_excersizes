{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Что такое эмбединги?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Термины \"векторизация\" или \"вложение\" (**embedding**) означают представление объекта в виде вектора в неком пространстве. Применяются они, как правило, к сложным нечисловым данным (таким как тексты, графы, последовательности), поскольку числовые данные и так без проблем представляются в виде вектора.\n",
    "\n",
    "Если мы хотим использовать нечисловые объекты как признаки в ML-моделях, такое представление необходимо.\n",
    "\n",
    "Самое простое векторное представление объекта это **one-hot encoding** (оно же **dummy encoding**). Работает так - мы перечисляем все возможные типы объектов и ставим 1 напротив нашего типа. Например, если мы последовательно кодируем три символа a,b и c, то one-hot encoding для каждого выглядит так:\n",
    "a=>[1,0,0], b=>[0,1,0], c=>[0,0,1]\n",
    "\n",
    "One-hot encoding иногда используют в задачах ML, но для текстов или других данных большой размерности оно крайне неээфективно => нужен другой способ кодирования.\n",
    "\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Word2Vec\n",
    "\n",
    "Word2Vec - алгоритм векторизации слова, при котором часто встречающиеся в одном контексте слова кодируются близкими векторами. Под контекстом слова понимаются слова, стоящие рядом слева или справа от него в предложении.\n",
    "\n",
    "Разработан в 2013 году Томасом Миколовым. Оригинал статьи: https://arxiv.org/abs/1301.3781\n",
    "\n",
    "Предполагается, что размерность эмбединга можно выбрать любой. Например, если получится закодировать слова двумерными векторами (embedding_dim = 2), то мы сможем делать красивые визуализации слов, в которых близкие по смыслу слова будут находиться близко на карте:\n",
    "\n",
    "<img src=\"img/wordmap.png\" width=500>\n",
    "\n",
    "Эмбединги не обязательно получать самому, можно загрузить готовые, полученные на текстах общего назначения (например, из википедии). Однако для лучшего качества моделей, рекомендуется все же обучать / дообучать эмбединги под каждую конкретную задачу.\n",
    "\n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2 Математическая постановка Word2Vec\n",
    "\n",
    "Мы понимаем, что вероятностное распределение каждого слова зависит от того, в каком контексте данное слово употребляется. И наоброт, веротяность контекста зависит от того какое слово, находится посередине.\n",
    "\n",
    "Например, для последовательности ('самый', \\_\\_\\_\\_\\_, 'корабль') распределение слова посередине может быть таким \n",
    "\n",
    "|WORD|P(WORD\\|CONTEXT)|\n",
    "|---|---|\n",
    "|быстрый|0.583|\n",
    "|красивый|0.127|\n",
    "|прочный|0.056|\n",
    "|...|...|\n",
    "|экзальтизм|0.0001|\n",
    "|крепеж|0.0001|\n",
    "\n",
    "Мы хотим найти модель веротяности каждого слова, которая лучше всего описывает наблюдаемые данные. То есть решаем задачу максимизации правдоподобия.\n",
    "\n",
    "При этом по условию задачи каждое слово у нас кодируется вектором, а вероятность рассчитывается как softmax от скалярного произведения эмбедингов word . Таким образом, мы хотим найти такое кодирование каждого слова, что все вместе они лучше всего описывают данные.\n",
    "\n",
    "Отметим, что существует две равнозначные постановки задачи:\n",
    "\n",
    "- **Continuous Bag-Of-Words (CBOW):** \n",
    "\n",
    "    $P(w_t|w_{con}) \\rightarrow max$\n",
    "    \n",
    "    Моделируем эмбединги слов так, чтобы каждое слово максимально точно угадывалось из контекста\n",
    "\n",
    "\n",
    "- **Skip-Gram:**  \n",
    "\n",
    "    $P(w_{con}|w_t) \\rightarrow max$\n",
    "    \n",
    "    Моделируем эмбдинги слов так, чтобы контекст максимально точно угадывался по центральному слову\n",
    "\n",
    "\n",
    "\n",
    "### 1.3 Техническая реализация Word2Vec\n",
    "\n",
    "Классическая реализация Word2Vec - простая нейронная сеть с одним скрытым слоем (hidden layer).\n",
    "\n",
    "Предположим, в нашем корпусе текстов мы насчитали 10000 различных слов и мы хотим смапить их в пространтсво размерности 300.\n",
    "\n",
    "Для начала мы нарезаем наш текст на скип-грамы скользящим окном. При обучении эмбедингов эти скип-грамы будут последовательно подаваться на вход сети. \n",
    "\n",
    "На картинке ниже показана сеть для модели SkipGram (когда на вход подается центральное слово, а выход сверяется с контекстом). Чуть позже увидим, чем отлиается вход/выход модели CBOW.\n",
    "\n",
    "<img src=\"img/word2vec.png\" width=500>\n",
    "\n",
    "Каждое входное слово кодируется с помощью one-hot encoding. То есть предварительно мы присваиваем каждому слову свой номер, а затем представляем его бинарным вектором размерности 10000, где единственная единица установлена напротив номера этого слова. То есть вход имеет примерно такой вид: (0, 0, 1, 0, ... 0, 0).\n",
    "\n",
    "<img src=\"img/nn.png\" width=300>\n",
    "\n",
    "Число нейронов в скрытом слое соотвествует размерности пространства эмбедингов (в нашем примере 300 штук).\n",
    "\n",
    "Матрица коэффиицентов W сожержит все веса для передачи сигнала от каждого входного слова до каждого нейрона скрытого слоя. То есть в нашем примере это матрица размером 10000 x 300 (всего 3000000 коэффиицентов).\n",
    "\n",
    "Обратим внимание, что на входе мы имеем 1 напротив только одного выбранного слова => выход скрытого слоя - это и есть эмбединг. То есть по сути, подавая на вход какое-то слово, мы просто делаем Lookup соотвествующей строки из матрицы W.\n",
    "\n",
    "Затем выход скрытого слоя нужно вернуть в исходное прострнатсво, чтобы увидеть, насколько хорошо мы предсказываем контекст. В выходном слое (softmax layer) мы имеем вектор веротятностей для каждого слова, что оно находится в том же контексте, что и входящее слово.\n",
    "\n",
    "Такой выход позволяет нам понять, насколько мы ошиблись и как сети нужно корректировать свои коэффициенты.\n",
    "\n",
    "Процесс получения эмбедингов очень прост:\n",
    "1. обучаем нейронную сеть на выборке текстов\n",
    "2. берем коэффициенты матрицы W, (то есть веса, с помощью которых входной сигнал мапится в скрытый слой) => это и есть эмбединги слов\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стоит отметить, что с технической точки зрения тут есть два подхода к реализации: \n",
    "- классический, описанный выше, с двумя матрицами W и W'\n",
    "- с одной матрицей весов W, когда мы делаем сиамскую сеть с двумя входами, которые затем просто перемножаются\n",
    "\n",
    "Второй подход с одной матрицей практичнее, поэтому когда будем делать реализацию (см. второй параграф) будем использовать именно его."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как было отмечено выше, есть две формулировки задачи Word2Vec:\n",
    "\n",
    "- **Skip-Gram**\n",
    "\n",
    "  на вход подается центральное слово и на выходе сверяется с его текущим контекстом\n",
    "\n",
    "\n",
    "- **CBOW** (continuous Bag-Of-Words)\n",
    "\n",
    "  на вход поадется контекст и на выходе мы сверяем его с центральным словом\n",
    "  \n",
    "Схематично оба подхода изображены на картинках из оригинальной статьи:\n",
    "<img src=\"img/sg_vs_cbow.png\" width=500>\n",
    "\n",
    "С точки зрения результатов, два подхода не сильно отличаются друг от друга и могут заменять друг друга.\n",
    "\n",
    "\n",
    "Обратите внимание, что в рамках модели Word2Vec вложение слова не зависит от соседних слов в кодируемом предложении - оно всегда одинаково. Это минус, когда мы имеем дело с омонимами (одно написание, но разные смыслы)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методика Word2Vec стала так поплярна в NLP, что породила большое количество аналогичных эмбедингов. Вот только некоторые:\n",
    "- Doc2Vec\n",
    "- Item2Vec\n",
    "- Graph2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Практика\n",
    "### 2.1 Реализация в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем сделать нейронную сеть для построения эмбедингов алгоритмом Word2Vec. Будем использовать API Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Reshape, merge, Dot\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.sequence import skipgrams\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import collections\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужен большой корпус текстов.\n",
    "\n",
    "У чувака есть предобработанный датасет с текстами из википедии (без знаков препинания). Весит около 30MB, можно скачать по прямой ссылке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'http://mattmahoney.net/dc/text8.zip'\n",
    "!unzip -o text8.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаться будем на 17 млн слов. \n",
    "\n",
    "Небольшой минус данного датасета - текст не делится на предложения и в конец статьи может попасть в начало другой при определении контекстов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17005207"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=open(\"text8\",\"r\")\n",
    "words = file.read().split()\n",
    "file.close()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитаем частоту слов. В питоне для этого есть специальный класс collections.Counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "word_freq = collections.Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ожидаемо наиболее частотные слова - артикли и числительные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дополним список специальным словом 'UNK' для обозначения более редких слов, чем с частотой n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = 10000\n",
    "word_freq_list = [['UNK', -1]]\n",
    "word_freq_list.extend(word_freq.most_common(n_words - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поставим в соотвествие каждому слову свой порядковый номер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = dict()\n",
    "for word, _ in word_freq_list:\n",
    "        dictionary[word] = len(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Закодируем текст в последовательность индексов слов. Заодно посчитаем частоту группы 'UKN'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "unk_count = 0\n",
    "for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "count[0][1] = unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наш текст выглядит так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потом когда будем анализировать результаты, нам наверняка понадобится обратный словарь (из кода слова в символьное представление)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "reversed_dictionary[1234]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зададим основные параметры обучения.\n",
    "- число слов в словаре (vocab_size), мы ранее сами задали ограничение в 10000 слов, поэтому пишем 10000\n",
    "- размер контекста слова (window_size), сколько сосдених слов будем брать с каждой стороны в качестве контекста слова\n",
    "- какую хотим получить размерность эмбединга (vector_dim)\n",
    "- число эпох для обучения нейросети (epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "window_size = 3\n",
    "vector_dim = 300\n",
    "epochs = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим частотное распределние слов для Negative Sampling. Для этого в Keras есть специальный метод sequence.make_sampling_table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_table = sequence.make_sampling_table(vocab_size)\n",
    "sampling_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в Keras есть метод для negative-sampling. Он генерирует положительные примеры (слово и слово из контекста) и отрицательные примеры (слово и произвольное слово). Слова выбираются исходя из частот полученных выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "couples, labels = skipgrams(data, vocab_size, window_size=window_size, sampling_table=sampling_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядят сэмплы примерно так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(couples[0:10], labels[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши пары слов на 2 отдельных списка. В питоне для этого есть функция zip(*).\n",
    "\n",
    "Мы нагенерировали 30 млн пар слов, поэтому обработка займет какое-то время (может 5-10 минут)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_target, word_context = zip(*couples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_target = np.array(word_target, dtype=\"int32\")\n",
    "word_context = np.array(word_context, dtype=\"int32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь будем собирать нейронную сеть. Схема ниже:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/word2vec_keras.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас 2 входа сети - один для анализируемого слова (target) и один для слова из его контекста (context). Оба - скаляры (код слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_target = Input((1,))\n",
    "input_context = Input((1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее каждый вход подключаем к эмбедингу (он один и тот же для обоих входов). \n",
    "\n",
    "В Keras слой Embedding - это просто словарик с набором описаний, элементы которого сеть считает весами. По мере обучения сети веса выстраиваются так, чтобы достигался минимальный loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding = Embedding(vocab_size, vector_dim, input_length=1, name='embedding')\n",
    "\n",
    "target = embedding(input_target)\n",
    "target = Reshape((vector_dim, 1))(target)\n",
    "\n",
    "context = embedding(input_context)\n",
    "context = Reshape((vector_dim, 1))(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перемножаем эмбединги обоих входов. Для скалярного произведения тензоров в Keras сущесвтует слой Dot.\n",
    "\n",
    "При создании слоя Dot ставим normalize = True. Это означает, что скалряное произведение становится косинусным антирасстоянием (cosine similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = Dot(axes=0, normalize=True)([target, context])\n",
    "dot_product = Reshape((1,))(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас softmax бинарная классификация (мы оцениваем вероятность события \"слово 1 входит в контекст слова 2\").\n",
    "\n",
    "Поэтому выход определяем как Dense слой со скалярным выходом и сигмоидной активацией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собираем и компилируем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input=[input_target, input_context], output=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем вариант модели с другой функцией билзости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_model = Model(input=[input_target, input_context], output=similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пара функций для визуального тестирования результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimilarityCallback:\n",
    "    def run_sim(self):\n",
    "        for i in range(valid_size):\n",
    "            valid_word = reversed_dictionary[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            sim = self._get_sim(valid_examples[i])\n",
    "            nearest = (-sim).argsort()[1:top_k + 1]\n",
    "            log_str = 'Nearest to %s:' % valid_word\n",
    "            for k in range(top_k):\n",
    "                close_word = reversed_dictionary[nearest[k]]\n",
    "                log_str = '%s %s,' % (log_str, close_word)\n",
    "            print(log_str)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_sim(valid_word_idx):\n",
    "        sim = np.zeros((vocab_size,))\n",
    "        in_arr1 = np.zeros((1,))\n",
    "        in_arr2 = np.zeros((1,))\n",
    "        in_arr1[0,] = valid_word_idx\n",
    "        for i in range(vocab_size):\n",
    "            in_arr2[0,] = i\n",
    "            out = model.predict_on_batch([in_arr1, in_arr2])\n",
    "            sim[i] = out\n",
    "        return sim\n",
    "    \n",
    "sim_cb = SimilarityCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждую эпоху дообучаем сеть на одном примере. Через каждые 10000 примеров для 16 случайно выбранных слов выводим его ближайшие слова.\n",
    "\n",
    "**Note1:** на ноутбуке сеть может обучаться очень долго. Для целей презентации подхода этого достаточно. Для достижения реального результата очень желательно запускать обучение на мощных серверах либо GPU.\n",
    "\n",
    "**Note2:** Чтобы сеть обучалась эффективнее, надо все же объединять сэмплы в батчи большего размера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_1 = np.zeros((1,))\n",
    "arr_2 = np.zeros((1,))\n",
    "arr_3 = np.zeros((1,))\n",
    "\n",
    "for cnt in range(epochs):\n",
    "    idx = np.random.randint(0, len(labels)-1)\n",
    "    arr_1[0,] = word_target[idx]\n",
    "    arr_2[0,] = word_context[idx]\n",
    "    arr_3[0,] = labels[idx]\n",
    "    loss = model.train_on_batch([arr_1, arr_2], arr_3)\n",
    "    if cnt % 100 == 0:\n",
    "        print(\"Iteration {}, loss={}\".format(cnt, loss))\n",
    "    if cnt % 10000 == 0:\n",
    "        sim_cb.run_sim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения модели можем выгрузить эмбединги слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embedding.get_weights()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем мапинг из слова в эмбединг:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2embedding = {w:embeddings[idx] for w,idx in dictionary.items()}\n",
    "word2embedding['love']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** эти эмбединги получены на текстах википедии. Разумеется, на других текстах они могут другими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Word2Vec в бибилотеке Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё что мы делали выше, можно записать одной строчкой, если воспользуемся NLP-библиоткеой gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Konstantin/anaconda/lib/python3.5/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В gensim класс Word2Vec работает с итераторами строк, а каждая строка - это список слов (но если массив небольшой, то это может быть и просто список списков слов).\n",
    "\n",
    "То есть в gensim токенизация и нормализация форм слов остается за пользователем.\n",
    "\n",
    "*В программировании итератор - обобщение понятия список. Если данные не влезают в память, гораздо эффективнее работать с указателями на данные (итераторами), чем с самими данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-90a8fbc99dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, cbow_mean, hashfxn, iter, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mbatch_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mhs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbow_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbow_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmin_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             fast_version=FAST_VERSION)\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_train_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sentences, workers, vector_size, epochs, callbacks, batch_words, trim_rule, sg, alpha, window, seed, hs, negative, cbow_mean, min_alpha, compute_loss, fast_version, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m             self.train(\n\u001b[1;32m    337\u001b[0m                 \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 end_alpha=self.min_alpha, compute_loss=compute_loss)\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrim_rule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    609\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[0;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# should be set by `build_vocab`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(common_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные параметры Word2Vec:\n",
    "- texts - то, на чем будем обучаться (список предложений)\n",
    "- size - размерность эмбединга\n",
    "- window - размер окна (в одну сторону)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(common_texts, size=100, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можем задать: \n",
    "- min_count\n",
    "- worker число параллельных процессов, если нам доступно много CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(common_texts, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, установив параметр \"sg\", мы можем выбрать модель обучения Word2Vec:\n",
    "- CBOW\n",
    "\n",
    "        Word2Vec(common_texts, sg=0)\n",
    "\n",
    "\n",
    "- Skip-gram\n",
    "\n",
    "        Word2Vec(common_texts, sg=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После обучения можем посмотреть embedding любого слова через индексатор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.vw['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.vw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обученные веса можно сохранить на диск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы также можем посчитать близость между предложениями (наборами слов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2228fcffd0d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sushi'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'shop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'japanses'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'restaurant'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.vw.n_similarity(['sushi','shop'],['japanses','restaurant'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
