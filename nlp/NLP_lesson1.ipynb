{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:blue\">**Занятие 1 (NLP)**</span>\n",
    "- <span style=\"color:gray\">Занятие 2 (NLP)</span>\n",
    "- <span style=\"color:gray\">Занятие 3 (NLP)</span>\n",
    "- <span style=\"color:gray\">Занятие 4 (Recommender Systems)</span>\n",
    "- <span style=\"color:gray\">Занятие 5 (Recommender Systems)</span>\n",
    "- <span style=\"color:gray\">Занятие 6 (CRISP-DM)</span>\n",
    "- <span style=\"color:gray\">Занятие 7 (to be anounced)</span>\n",
    "- <span style=\"color:gray\">Занятие 8 (to be anounced)</span>\n",
    "- <span style=\"color:gray\">Занятие 9 (Class Hours)</span>\n",
    "- <span style=\"color:gray\">Занятие 10 (Class Hours)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты можно использовать в задачах машинного обучения точно так же, как обычные числовые данные. Единственное требование - нужно как-то представить их в виде числового вектора.\n",
    "\n",
    "На этом занятии мы посмотрим, какие есть подходы для векторизации текстов и увидим, как эти подходы используются в контексте решения практических задач класификации.\n",
    "\n",
    "#### Requirements\n",
    "- sklearn\n",
    "- nltk\n",
    "\n",
    "#### Глоссарий\n",
    "- <span style=\"color:blue\">Document</span> - единица текста, соотвествующая одному наблюдению (статья, отзыв, предложение)\n",
    "- <span style=\"color:blue\">Corpus</span> - набор всех документов, которыми мы располагаем\n",
    "- <span style=\"color:blue\">Term</span> или <span style=\"color:blue\">Token</span> - составная часть, на которую бъется документ (например, слово)\n",
    "- <span style=\"color:blue\">Document-Term</span> матрица - матрица, у которой по строкам отложены документы, а по столбцам термы, значением же является частота данного терма в документе (или другие характеристики). Также иногда работают с ее транспонриованным вариантом Term-Document matrix.\n",
    "- <span style=\"color:blue\">Vocabulary</span> - набор всех термов из корпуса и назначенных им порядковых номеров\n",
    "- <span style=\"color:blue\">Vector-Space Model</span> - способ представления терма в виде многомерного вектора\n",
    "- <span style=\"color:blue\">Term Frequency</span> - частота терма в данном документе\n",
    "- <span style=\"color:blue\">Document Frequency</span> - в каком проценте документов корпуса встречается данный терм\n",
    "- <span style=\"color:blue\">Inverse Docuemnt Frequency</span> - 1/DF, коэффициент \"уникальности\" терма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим список из 3 документов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['To be, or not to be: that is the question:','to be be to', 'I went to party yesterday']\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Попробуем его векторизовать. Для этого создаем объект класса **CountVectorizer** из библиотеки sklearn. В конструкторе пока используем все настройки по умолчанию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс CountVectorizer сождержит 2 классических для sklearn метода \n",
    "- fit() - построение словаря\n",
    "- transform() - преобразование текста в соответствии с этим словарем\n",
    "\n",
    "Поскольку часто эти действия выполняются одно за другим, есть еще метод\n",
    "- fit_transform()\n",
    "\n",
    "который выполняет их сразу оба. Этот метод обычно и используется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_vectorized = vect.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате вызова метода fit_transform заполняется словарь термов. Мы можем посмотреть, что в нем. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что каждому уникальному терму назначен свой порядковый номер (0,1,2...) (часто порядковые номера назначаются по мере того, как новые слова встречаются в документе, однако порядок нумерации здесь не имеет никакого значения, поэтому  подобное представление текста называется bag-of-words).\n",
    "\n",
    "Кроме того на выходе мы получаем Document-Term матрицу (то есть набор векторизированных текстов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vectorized.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность получившейся document-term матрицы (3,11). Действительно, у нас 3 документа и можем посчитать, что в этих текстах 11 уникальных слов (местоимение \"I\" по умолчанию отфильтровывается как незначимое, так что оно не в счет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(text_vectorized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на выходе у нас [sparse-матрица](https://en.wikipedia.org/wiki/Sparse_matrix). Такой формат позволяет эффективно хранить матрицы огромных размерностей, но из-за этого мы не можем отобразить ее на экране как обычную матрицу. Так как матрица небольшая, мы можем явно преобразовать её в dense-матрицу и вывести на экран."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как у нас **Count**Vectorizer, в результирующем векторе мы видим **число** вхождений кажого терма."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как еще можно представлять текст.\n",
    "\n",
    "Например, вместо слов часто используют 2-граммы. Для этого при создании векторайзера используем параметр ngram_range. Обратите внимание, что параметр должен быть типа Tuple из 2 элментов. Он задает интервал длин n-граммов, которые будут рассчитаны. В примере ниже считаются только 2-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(2,2))\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что словарь выглядит уже по-другому - он состоит из всех уникальных последовательных пар слов, которые встречаются в тексте. \n",
    "\n",
    "Преобразованный с помощью этого словря текст представляет собой 3 вектора в пространстве большей размерности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На практике часто комбинируют n-граммы разной размерности. При построении моделей это позволяет находить паттерны не только в словах, но и в словосочетаниях и устойчивых выражениях. \n",
    "\n",
    "Ниже пример, как рассчитать 1-граммы и 2-граммы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2))\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По умолчанию тексты переводятся в нижний регистр. Иногда регистр важен, в этом случае, его можно отключить опцией lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(lowercase=False)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда с текстами работают не на уровне слов или n-грамов, а на уровне символов (это встречается, например, в задачах генерации). Мы можем задать, на что будем разбивать наши документы, параметром analyzer - для символов это analyzer='char', а для слов (по умолчанию) это analyzer='word'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(analyzer='char')\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также можем явно задать фильтрацию неинформативных стоп-слов. Они передаются списком."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words=['to','a','then'])\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что отфильтровалось слово to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важная функция - мы можем отфильтровать термы со слишком большой или слишком малой частотой использования в корпусе (document frequency).\n",
    "\n",
    "Большой DF говорит о том, что слово скорее всего малоинформативно - оно характерно для языка в целом, но не характерно для каких-либо классов документов (а это нехорошо, если мы захотим решать задачу классификации на этих данных). С большим DF обычно предоги, союзы и общеупотребительные слова и выражения.\n",
    "\n",
    "Термы с малым DF слабо влияют на целевую переменную, занимая при этом место. Их можно спокойно отфильтровать.\n",
    "\n",
    "Эти пороги задает пара параметров: min_df и max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=0.01, max_df=0.5)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы не хотим хранить и каждый раз обновлять словарь, можем воспользоваться алгоритмом хэширования слов. Аналогом класса CountVectorizer с реализованным hashing trick является класс **HashingVectorizer**. Определение индекса слова в векторном представлении документа просиходит динамически с помощью хэш-функции.\n",
    "\n",
    "Обратите внимание, что по умолчанию, HasingVectorizer нормирует получаемый вектор документа. Для иллюстрации пока отключим нормировку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vect = HashingVectorizer(norm=None, alternate_sign=False)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание на размерность данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не видно. Чтобы найти индексы наших 8 термов можно сделать так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.nonzero(text_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Индексы знаем, теперь можно посмотреть сами значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vectorized[numpy.nonzero(text_vectorized)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один часто используемый векторайзер это класс **TfidfVectorizer**. Он является полным аналогом CountVectorizer, но по умолчанию включает TF-IDF преобразование частот."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(norm=False)\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vect = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "text_vectorized = vect.fit_transform(text)\n",
    "\n",
    "print(text_vectorized.shape)\n",
    "print(type(text_vectorized))\n",
    "text_vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы sklearn предоставляют базовый набор возможностей для предобработки текстовых данных. Когда нужно что-то более сложное, либо пишут обработчики самостоятельно, либо прибегают к использованию сторонних библиотек (например, NLTK).\n",
    "\n",
    "Типичный набор преобразований текста выглядит так: \n",
    "- нормализация (удаление неинформативных символов, цифр)\n",
    "- токенизация (разделение текста на смысловые единицы)\n",
    "- лемматизация (приведение слова к нормальной форме, именительный падеж, единственное число)\n",
    "- стемминг (выделение основы слова)\n",
    "- выделение частей речи\n",
    "- фильтрация стоп-слов (соединительных слов, союзов и предлогов, специфичных для доменной области неинформативных слов)\n",
    "\n",
    "и так далее.\n",
    "\n",
    "Далее несколько примеров преобразований из библиотеки NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "doc = word_tokenize('Я пришел в столовую и съел волшебных котлет')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Only English\n",
    "lemmatizer.lemmatize(\"fugitives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "tags = pos_tag(doc, lang='rus')\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что тэгер каждое слово текста относит к определенной части речи (V - глагол, CONJ - союз, PR - предлог и так далее). \n",
    "\n",
    "В основе тэгера лежит самообучаемая предиктивная модель. Она не использует словари, поэтому вполне работает и с выдуманными языком"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = pos_tag(word_tokenize('кареокий шляндр высвестил поорекомандно кнодлик'), lang='rus')\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классы CountVectorizer, HashingVectorizer и TfidfVectorizer допускают кастомизацию. Например, мы можем реализовать свою логику обработки в специальном классе MyTokenizer.\n",
    "\n",
    "Вся логика обработки реализуется в методе \\__call\\__, куда в качестве параметра передается документ для обработки (doc). Ниже пример такого класса из одного из проектов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = Mystem()\n",
    "    def __call__(self, doc):\n",
    "        tokens = word_tokenize(doc)\n",
    "        pos = pos_tag(tokens, lang='rus')\n",
    "        pos = [x[0] for x in pos if (x[1] not in [\"NONLEX\",\"CONJ\"]) or (x[0] == 'eos')]\n",
    "        lemmatized_tokens = [self.wnl.lemmatize(t)[0] for t in pos]\n",
    "        return (lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экземпляр этого класса передается в качестве параметра при создании векторайзера:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(\n",
    "    tokenizer = MyTokenizer(), \n",
    "    min_df=0.001,\n",
    "    max_df=0.75, \n",
    "    stop_words = russian_stop_words, \n",
    "    lowercase=True, \n",
    "    ngram_range=(1,2))\n",
    "\n",
    "text_vectorized = cv.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем решить реальную задачу. Для этого загрузим датасет с текстами с помощью встроенных средств sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "text_dataset = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете содержатся тексты 20 категорий. Их названия можно прочитать в переменной target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужно перевести тексты в векторное представление!\n",
    "\n",
    "Повторяем то, что делали выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(text_dataset.data)\n",
    "X_train_counts.shape\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для классификации будем использовать Multinomial NaiveBayes классификатор. Это простейший классификатор среди тех, которые обычно применяют для классификации текстовых данных. Он основан на подчсете частотности появления слов в каждом классе.\n",
    "\n",
    "В метод fit() передаем обучающую выборку: матрицу предкиторов и целевую переменную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, text_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает классификатор на 2 тестовых примерах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
    "X_new_counts = count_vect.transform(docs_new)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs_new' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2164ebd0f9b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%r => %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs_new' is not defined"
     ]
    }
   ],
   "source": [
    "for doc, category in zip(docs_new, predicted):\n",
    "    print('%r => %s' % (doc, text_dataset.target_names[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательности трансформаций удобно объединять в пайплайны. Потом их можно обучать, так же как обычный классификатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([\n",
    "     ('vect', CountVectorizer()),\n",
    "     ('tfidf', TfidfTransformer()),\n",
    "     ('clf', MultinomialNB()),\n",
    "])\n",
    "text_clf.fit(text_dataset.data, text_dataset.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делаем прогноз на тестовой части данных и проверяем его качество. Используем метрику Accuracy - показывает, в каком проценте тестовых кейсов угадываем правильный ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-74db29c360e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Apply Model to new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Assess Quality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_clf' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Test Dataset\n",
    "text_dataset_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
    "docs_test = text_dataset_test.data\n",
    "\n",
    "# Apply Model to new data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "\n",
    "# Assess Quality\n",
    "import numpy as np\n",
    "np.mean(predicted == text_dataset_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем использовать алгоритм классификации SVM, который также является стандартным алгоритмом (эпохи доглубокого обучения) для работы с текстами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "text_clf.fit(text_dataset.data, text_dataset.target)  \n",
    "predicted = text_clf.predict(docs_test)\n",
    "numpy.mean(predicted == text_dataset_test.target)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможно сущесвтуеют параметры, используя которые, мы можем повысить точность модели? Узнать можно только одним способом - попробовать. Для этого сущесвтует такой механизм, как Grid Search. Это способ запуска обучения, при котором в цикле перебираются возможные наборы параметров, а затем находится оптмаильный.\n",
    "\n",
    "Для начала необходимо создать сетку параметров, на которой мы будем искать оптимум.\n",
    "\n",
    "По каждому параметру мы просто списком перечисляем значения, которые мы хотим, чтобы он принимал.\n",
    "\n",
    "Можно задавать как непосредственно параметры моделей, так и параметры трансформаций. Для этого достаточно указать имя шага (как в пайплайне) и через два подчеркивания имя параметра."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для автоматизации процесса обучения на сетке параметров в sklearn существует класс GridSearchCV. Его параметры:\n",
    "- Классификатор или пайплайн, который нужно запускать \n",
    "- Сетка параметров\n",
    "- Вид кросс-валидации. Например, если cv=5, то будет использована 5-кратная k-fold валидация. Кросс-валидация нужна для получения более устойчивых метрик качества по каждому набору парметров. Значение метрики усредняется между этих 5 запусков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "cvGrid = GridSearchCV(text_clf, parameters, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Созданная выше сетка предполагает 2^3 = 8 различных комбинаций параметров.\n",
    "\n",
    "CV = 5 запускает процесс обучения 5 раз\n",
    "\n",
    "Итого, требуется 5 * 8 = 40 запусков. Расчет может занять продолжительное время. Попробуем запуститься на небольшом помножестве выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.fit(text_dataset.data[:400], text_dataset.target[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как посмотреть наилучшее значение метрики?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть словарик cv results_, который содержит всю информацию об обучении. Для нас наиболее важен массив mean_test_score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from Для сравнения попробуем обучаться всего на 4 отобранных категориях, как в документации. Это сократит объем обучающей выборки, и считаться будем быстрее. Кроме того, они мало пересекаются, поэтому можно ожидать, что точность прогноза будет еще выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...   penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
       "       verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "df = fetch_20newsgroups(subset=\"train\")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vectorized_text = vect.fit_transform(df['data'])\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', SGDClassifier())])\n",
    "\n",
    "text_clf.fit(df.data, df.target)\n",
    "\n",
    "\n",
    "# docs_test = text_dataset_test.data\n",
    "# predicted = text_clf.predict(docs_test)\n",
    "# numpy.mean(predicted == text_dataset_test.target)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x130107 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1787565 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так и есть\n",
    "\n",
    "Теперь повторим GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "     'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "     'tfidf__use_idf': (True, False),\n",
    "     'clf__alpha': (1e-2, 1e-3),\n",
    "}\n",
    "cvGrid = GridSearchCV(text_clf, parameters, cv=5)\n",
    "cvGrid.fit(text_dataset.data, text_dataset.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность выше, чем на полном наборе данных.\n",
    "\n",
    "Посмотрим полную информацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvGrid.cv_results_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
