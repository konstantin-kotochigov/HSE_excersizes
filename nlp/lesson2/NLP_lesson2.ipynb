{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <span style=\"color:black\">Занятие 1 (NLP)</span>\n",
    "- <span style=\"color:blue\">**Занятие 2 (NLP)**</span>\n",
    "- <span style=\"color:gray\">Занятие 3 (NLP)</span>\n",
    "- <span style=\"color:gray\">Занятие 4 (Recommender Systems)</span>\n",
    "- <span style=\"color:gray\">Занятие 5 (Recommender Systems)</span>\n",
    "- <span style=\"color:gray\">Занятие 6 (CRISP-DM)</span>\n",
    "- <span style=\"color:gray\">Занятие 7 (to be anounced)</span>\n",
    "- <span style=\"color:gray\">Занятие 8 (to be anounced)</span>\n",
    "- <span style=\"color:gray\">Занятие 9 (Class Hours)</span>\n",
    "- <span style=\"color:gray\">Занятие 10 (Class Hours)</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review предыдущего занятия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты можно использовать в задачах машинного обучения точно так же, как обычные числовые данные. Единственное требование - нужно как-то представить их в виде числового вектора.\n",
    "\n",
    "- <span style=\"color:blue\">Document</span> - единица текста, соотвествующая одному наблюдению (статья, отзыв, предложение)\n",
    "- <span style=\"color:blue\">Corpus</span> - набор всех документов, которыми мы располагаем\n",
    "- <span style=\"color:blue\">Term</span> или <span style=\"color:blue\">Token</span> - составная часть, на которую бъется документ (например, слово)\n",
    "- <span style=\"color:blue\">Document-Term</span> матрица - матрица, у которой по строкам отложены документы, а по столбцам термы, значением же является частота данного терма в документе (или другие характеристики). Также иногда работают с ее транспонриованным вариантом Term-Document matrix.\n",
    "- <span style=\"color:blue\">Vocabulary</span> - набор всех термов из корпуса и назначенных им порядковых номеров\n",
    "- <span style=\"color:blue\">Vector-Space Model</span> - способ представления терма в виде многомерного вектора\n",
    "- <span style=\"color:blue\">Term Frequency</span> - частота терма в данном документе\n",
    "- <span style=\"color:blue\">Document Frequency</span> - в каком проценте документов корпуса встречается данный терм\n",
    "- <span style=\"color:blue\">Inverse Docuemnt Frequency</span> - 1/DF, коэффициент \"уникальности\" терма\n",
    "\n",
    "В библиотеке Sklearn есть готовый датасет для работы с текстами (fetch_20newsgroups()), а также 3 класса для векторизации текстов: \n",
    "- CountVectorizer() - для простоты и скорости работы\n",
    "- HashingVectorizer() - для эффективной работы с часто обновляющимся словарем\n",
    "- TfidfVectorizer() - для улучшения качества предсказания предиктивных моделей, работающих с этими текстами\n",
    "\n",
    "Также есть трансформация TfidfTransfomer(), которую можно применять отдельно над уже переведенным в векторное представление текстом для перевзвешивания признаков.\n",
    "\n",
    "Типичный стек преобразований текста:\n",
    "- нормализация (удаление неинформативных символов, цифр)\n",
    "- токенизация (разделение текста на смысловые единицы)\n",
    "- лемматизация (приведение слова к нормальной форме, именительный падеж, единственное число)\n",
    "- стемминг (выделение основы слова)\n",
    "- выделение частей речи\n",
    "- фильтрация стоп-слов (соединительных слов, союзов и предлогов, специфичных для доменной области неинформативных слов)\n",
    "\n",
    "POS_tagging - задача определения частей речи каждого слова\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии обсуждали важность TF-IDF преобразования. Данный принцип можно сформулировать в более общем виде: при сравнении двух объектов совпадение \"редких\" атрибутов гораздо больше говорит о схожести этих объектов, чем совпадение часто втсречающихся атрибутов.\n",
    "\n",
    "Посмотрим как tf-idf взвешивание может влиять на точность модели классификации. Добавим в сетку параметров два значения: tf-idf включено/выключено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## О важности TF-IDF преобразования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии обсуждали важность TF-IDF преобразования. Данный принцип можно сформулировать в более общем виде: при сравнении двух векторов совпадение \"редких\" элементов гораздо больше говорит о схожести этих объектов, чем совпадение часто втсречающихся атрибутов.\n",
    "\n",
    "Посмотрим как tf-idf взвешивание может влиять на точность модели классификации. Добавим в сетку параметров два значения: tf-idf включено/выключено"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mean_fit_time': array([0.47783971, 0.38655   ]), 'std_fit_time': array([0.11315002, 0.01208689]), 'mean_score_time': array([0.10601983, 0.08410416]), 'std_score_time': array([0.03947273, 0.00399529]), 'param_vect__use_idf': masked_array(data=[True, False],\n",
      "             mask=[False, False],\n",
      "       fill_value='?',\n",
      "            dtype=object), 'params': [{'vect__use_idf': True}, {'vect__use_idf': False}], 'split0_test_score': array([0.96460177, 0.9159292 ]), 'split1_test_score': array([0.96238938, 0.8960177 ]), 'split2_test_score': array([0.96230599, 0.92904656]), 'split3_test_score': array([0.96452328, 0.9135255 ]), 'split4_test_score': array([0.97339246, 0.93569845]), 'mean_test_score': array([0.96544258, 0.91804348]), 'std_test_score': array([0.00409666, 0.01373159]), 'rank_test_score': array([1, 2], dtype=int32)}\n",
      "[0.96544258 0.91804348]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Load datasets\n",
    "categories_to_load = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "text_train = fetch_20newsgroups(subset='train', categories = categories_to_load, shuffle=True)\n",
    "text_test = fetch_20newsgroups(subset='test', categories = categories_to_load, shuffle=True)\n",
    "\n",
    "# Define classification pipeline (vectorizer + classifier)\n",
    "text_clf = Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-3, random_state=42,\n",
    "                           max_iter=5, tol=None))])\n",
    "\n",
    "# Define parameters grid\n",
    "parameters = {\n",
    "     'vect__use_idf': (True, False)\n",
    "}\n",
    "\n",
    "cvGrid = GridSearchCV(text_clf, parameters, cv=5)\n",
    "\n",
    "cvGrid.fit(text_train.data, text_train.target)\n",
    "\n",
    "# When fitting is done, study the structure of \"cv_result_\" dictionary\n",
    "print(cvGrid.cv_results_)\n",
    "\n",
    "# Print values of scoring metric for each parameter combination\n",
    "print(cvGrid.cv_results_['mean_test_score'])           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что при наличии tf-idf взвешивания точность предсказательной модели выросла почти на 4 процента, что для задач машинного обучения очень существенно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение текстовых векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что при наличии tf-idf взвешивания точность предсказательной модели выросла почти на 4 процента, что для задач машинного обучения очень существенно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, мы научились представлять тексты в виде векторов в пространстве слов. Как их сравнивать между собой.\n",
    "\n",
    "Чтобы оценить \"схожесть\" двух текстов чаще всего используются два расстояния:\n",
    "- [косинусное расстояние](https://en.wikipedia.org/wiki/Cosine_similarity) (cosine distance)\n",
    "\n",
    "    Косинусное расстояние может быть посчитано для любой пары векторов в исследуемом пространстве.\n",
    "    \n",
    "    $$\\text{similarity} = \\cos(\\theta) = {\\mathbf{A} \\cdot \\mathbf{B} \\over \\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i  B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{A_i^2}}  \\sqrt{\\sum\\limits_{i=1}^{n}{B_i^2}} }$$\n",
    "    \n",
    "    <img src=\"img/cosine_plot.png\" width=300>\n",
    "\n",
    "\n",
    "- [мера Жаккара](https://en.wikipedia.org/wiki/Jaccard_index) (Jaccard index)\n",
    "\n",
    "    <img src=\"img/jaccard_plot.jpg\" width=350>\n",
    "    \n",
    "    Эта мера выводит число слов, которые присутствуют и в тексте A, и в тексте B, поделенное на число слов, которые есть или в тексте A, или в тексте B (то есть общее число слов в двух текстах).\n",
    "\n",
    "    Мера Жаккара считается на множествах, поэтому может быть применена только к бинарным векторам (любой бинарный вектор представляет некоторое множество, например {1,0,0,1,0}, где 1 означает, что этот элемент входит в множество, а 0 - не входит).\n",
    "    \n",
    "    Таким образом, если строка описывается не бинарным векторным представлением (где каждый элемент означает, что слово входит или не входит), а например частотами слов, этот вектор надо сначала бинаризовать.\n",
    "\n",
    "В математике многие метрики представляют в двух вариантах:\n",
    "- близость (similarity), когда меняется от 0.0 (полное несовпадение) до 1.0 (полное совпадение)\n",
    "- расстояние (distance), когда меняется от 0.0 (полное совпадение) до 1.0 (полное несовпадение)\n",
    "\n",
    "Одно легко получается из другого вычитанием: similarity = 1 - distance. При чтении материалов важно не путать, о каком из двух вариантов метрики идет речь.\n",
    "\n",
    "### Реализация в Sklearn\n",
    "\n",
    "#### Cosine\n",
    "\n",
    "В Sklearn можно посчитать косинусное расстояние (оба варианта метрики). Для этого в модуле <b>metrics.pairwise</b> есть функции \n",
    "- cosine_similarity(X,Y)\n",
    "- cosine_distances(X,Y)\n",
    "\n",
    "Эти функции умеют считать расстояния не только между парой векторов, но и могут вывести все попарные расстояния между двумя наборами векторов. Поэтому аргументы X и Y могут быть матрицами.\n",
    "\n",
    "Если X  - набор векторов длины m (X.shape[0]==m), в Y - набор векторов длины n (Y.shape[0]==n), то на выходе получаем матрицу расстояний размерности (m x n).\n",
    "\n",
    "#### Jaccard\n",
    "\n",
    "Меру Жаккара можно посчитать функцией [sklearn.metrics.jaccard_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html).\n",
    "\n",
    "Эта метрика не из модуля sklearn.metrics.pairwise, поэтому ее можно посчитать только для пары векторов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск ближайших соседей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем вывести все попарные расстояния для первых 100 документов нашей выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Переводим тексты в вектороное представление\n",
    "vect = TfidfVectorizer()\n",
    "vectorized_text = vect.fit_transform(text_train['data'])\n",
    "\n",
    "# Вычисляем матрицу расстояний\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "sim_matrix = cosine_similarity(vectorized_text[0:100], vectorized_text[0:100])\n",
    "sim_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим рассчитанные значения косинусного расстояния. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.03673497, 0.05673791, 0.0174853 , 0.0429553 ,\n",
       "       0.04380125, 0.028339  , 0.03478446, 0.02329201, 0.0265978 ,\n",
       "       0.06227516, 0.05040917, 0.05015605, 0.02212076, 0.02852566,\n",
       "       0.01073885, 0.02376148, 0.01664064, 0.02358362, 0.02110355,\n",
       "       0.01595373, 0.03403637, 0.02214675, 0.0152878 , 0.07751104,\n",
       "       0.0116598 , 0.01095673, 0.020069  , 0.01312511, 0.03081208,\n",
       "       0.05077404, 0.04798343, 0.02879682, 0.03533289, 0.03344134,\n",
       "       0.01479038, 0.026262  , 0.03299799, 0.04018106, 0.0253074 ,\n",
       "       0.01686755, 0.0305604 , 0.02093238, 0.02697748, 0.03021066,\n",
       "       0.04073356, 0.03039813, 0.05718717, 0.01759402, 0.02020483,\n",
       "       0.09577765, 0.0203808 , 0.02011274, 0.03864396, 0.02860147,\n",
       "       0.05271046, 0.03936306, 0.06122792, 0.02305622, 0.03975851,\n",
       "       0.0463685 , 0.02608623, 0.00614942, 0.03228326, 0.01577141,\n",
       "       0.01938515, 0.03979195, 0.01991409, 0.0398553 , 0.03307226,\n",
       "       0.0398576 , 0.01882212, 0.01662219, 0.03276784, 0.05187862,\n",
       "       0.03094711, 0.03576414, 0.06603587, 0.04017871, 0.05234687,\n",
       "       0.03255935, 0.03110878, 0.02821164, 0.01793087, 0.02518721,\n",
       "       0.03561112, 0.02255728, 0.02170296, 0.05382228, 0.02801207,\n",
       "       0.02519165, 0.02777623, 0.027534  , 0.0347091 , 0.0286078 ,\n",
       "       0.03589201, 0.00765788, 0.04805749, 0.03913331, 0.02276173])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_matrix[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что на диагонали всегда стоит 1.0 - совпадение документа с самим собой всегда максимально. Какие-то пары документов больше похожи друг на друга, какие-то меньше. \n",
    "\n",
    "Имея матрицу расстояний, можем для любого текста найти наиболее похожие на него тексты.\n",
    "Такое действие может лежать в основе например:\n",
    "- системы поиска дубликатов на сайте объявлений\n",
    "- системы определения плагиата в научных работах\n",
    "- системы рекомендаций статей в агрегаторе изданий\n",
    "- системы рекмендаций фильмов, содержание которых описываются некоторым текстом\n",
    "\n",
    "Выведем номера первых пяти документов, наиболее сходных с запрашиваемым:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sim_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-16dc14387807>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Расстояния до остальных текстов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mquery_neighbors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_document_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mquery_neighbors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sim_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "# Для какого текста ищем соседей\n",
    "query_document_num = 10\n",
    "\n",
    "# Расстояния до остальных текстов\n",
    "query_neighbors = sim_matrix[query_document_num,:]\n",
    "query_neighbors\n",
    "\n",
    "# Выводим индексы документов с 5 наименьшими расстояниями\n",
    "sorted(range(len(query_neighbors)), key=lambda x: query_neighbors[x], reverse=True)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация текстов\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тексты можно использовать в качестве признаков и в __unsupervised__ задачах, например, в задаче кластеризации.\n",
    "\n",
    "Типичные кейсы:\n",
    "- клиенты пишут заявки в службу техподдержки; в рамках анализа статистики запросов нужно определить основные темы обращений \n",
    "- покупатели оставляют отзывы на товар; основные темы отзывов\n",
    "- новостной агрегатор; необходимо сгруппировать статьи в тематические блоки и отфильтровать дубликаты\n",
    "\n",
    "Вообще, векторное представление характерно не только для текстов, так могут представляться любые наборы категорийных атрибутов. \n",
    "\n",
    "Примеры задач:\n",
    "- анализируется аудитория посетителей веб-сайта, и каждый посетитель описывается с помощью своего поведенческого профиля (например, интерес \"спорт\":1, интерес \"путешествия\":10, интерес \"машины\" :6); в рамках задачи персонализации стратегии коммуникации с клиентами необходимо разбить клиентскую базу на 5 однородных кластеров\n",
    "- сгруппровать пользователей по списку скачанных приложений в несколько кластеров, описать каждый и по каждому рекомендоватьсвой тарфиный план"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для кластеризации загрузим выборку из датасетов Sklearn (тексты из тематических групп)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_load = ['sci.space','talk.politics.mideast','rec.autos']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "text_dataset = fetch_20newsgroups(subset='train', categories=categories_to_load, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сообщения там подобного вида"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_dataset['data'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Помним, что для работы с текстами, их сначала нужно представить в виде векторов. Для этого используем уже знакомый TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect = TfidfVectorizer()\n",
    "vectorized_text = vect.fit_transform(text_dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сделаем обратное преобразование: посмотрим, какие слова закодированы в первом предложении. Для этого выделим ненулевые каунты из получившейся document-term матрицы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonzero_dimensons = numpy.nonzero(vectorized_text[0])\n",
    "print(nonzero_dimensons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Координаты ненулевых элементов возвращаются двумя списками - сначала координаты строк, затем координаты стоблцов. Мы взяли только первую строку Document-Term матрицы, поэтому в первом списке везде 0. Теперь хотим координаты первести в соотвестующие им слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Словарь, который мы заполнили, вызывая метод fit(), имеет формат {term => dimension}. Но нам наоборот нужно по номеру измерения достать кодируемое слово {dimension=>term}. Воспользуемся одним питоновским финтом, чтобы инвертировать словарь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "inverse_vocabulary = {v:k for k,v in vect.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Теперь словарь выглядит так (возьмем 10 записей из середины)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(inverse_vocabulary.items())[10000:10010])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "и можно применить функцию поиска в словаре по ключу (метод get) к нашему списку непустых элементов. Список слов выглядит так (первые 20 штук):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(inverse_vocabulary.get, list(nonzero_dimensons[1])))[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Или удобно использовать уже инвертированный словарь, который разработчики добавили в класс TfidfVectorizer. Результат тот же."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [vect.get_feature_names()[x] for x in nonzero_dimensons[1]][0:20]\n",
    "# vect.vocabulary_\n",
    "vect.get_feature_names()http://localhost:8888/notebooks/git/lesson2/NLP_lesson2.ipynb#\n",
    "# nonzero_dimensons[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всегда рекомендуется устанавливать параметры min_df и max_df. Это сильно сократит размерность, а также избавит от неифномартивных термов.\n",
    "\n",
    "Параметры можно ставить как доли от общего числа документов в корпусе, либо как абсолютные значения.\n",
    "\n",
    "Оптимальные пороги определяются методом проб - либо с помощью GridSearchCV, либо визуально при исследовании результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(min_df=0.05, max_df=0.5)\n",
    "vectorized_text = vect.fit_transform(text_dataset['data'])\n",
    "len(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кластеризация алгоритмом K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем применить наиболее популярный алгоритм кластеризации K-means.\n",
    "\n",
    "K-means - итеративный алгоритм. Сходимость алгоритма проиллюстрирована на картинке ниже:\n",
    "<img src=\"img/kmeans.png\" width=700>\n",
    "\n",
    "Алгоритм следующий:\n",
    "1. Случайным образом выделяем центры наших 2 кластеров (b)\n",
    "2. Относим каждую точку в тот кластер, к центру которого она ближе (c)\n",
    "3. Пересчитываем центры внось полученных кластеров (d) и повторяем с пункта 1\n",
    "\n",
    "Через некоторе кол-во шагов алгоритм сойдется к решению.\n",
    "\n",
    "NOTE: как это очень часто бывает в машинном обучении, из-за случайного первого приближения, результаты могут различаться от запуска к запуску. Нужно всегда иметь это в виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что в k-means, как и в большинстве алгоритмов кластерзиации, число кластеров задается вручную до запуска алгоритма:\n",
    "- n_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(vectorized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем номер кластера, в который относим точку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывести центры кластеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание кластеров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельная задача, выполняемая в рамках кластерзиации - **описание полученных кластеров**.\n",
    "\n",
    "Один из способов - посчитать метрику **uplift**, то есть насколько чаще слово встречается в рамках данного кластера, чем в остальных кластерах. И затем по каждому кластеру вывести набор слов с наибольшим аплифтом. Эти слова и будут лучше всего выделять данную группу текстов.\n",
    "\n",
    "$$ \\text{Uplift(w,}C_{k}\\text{)} = \\frac{|w_{k}|}{|C_{k}|} / \\frac{|w_{other}|}{|C_{other}|}$$\n",
    "\n",
    "где:\n",
    "- $|w_{k}|$ - число документов в кластере $C_k$, в которых встречается слово $w$\n",
    "- $|w_{other}|$ - число документов в остальных кластерах, в которых встречается слово $w$\n",
    "- $|C_{k}|$ - число документов в кластере $C_k$\n",
    "- $|C_{other}|$ - число документов в остальных кластерах"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Моделирование тематики текста - другой пример обучения без учителя. Основное отличие от кластерзиации - текст не обязательно относится только к одному из кластеров, он может содержать сразу нескольких тематик. \n",
    "\n",
    "На практике бывают ситуации, когда кластеризация дает слишком глубое решение, особенно если нет одной ярко выраженной темы, но есть комбинация нескольких тематик. В этих случаях прибегают к тематическому моделированию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Latent Semantic Analysis ](https://en.wikipedia.org/wiki/Latent_semantic_analysis) (LSA) - общее название для методов выделения тематик в тексте.\n",
    "\n",
    "#### Почнму семантический?\n",
    "Потому что цель анализа - выделение смысла (семантики) в тексте\n",
    "\n",
    "#### Почему латентный?\n",
    "В машинном обучении так называют ситуации, когда при моделировании какого-то процесса присутствует некая ненаблюдаемая (латентная) переменная, которая объясняет суть процесса, но о ее конкретных значениях мы можем только делать предположения. В случае с текстами такой латентной переменной обычно называют как раз тематику текста - она влияет на то, какие слова используются в тексте, но мы её не знаем. Задача LSA как раз сделать предположение относительно тематики.\n",
    "\n",
    "#### Теоретическое введение\n",
    "Любой сложный сигнал (в том числе временные ряды, картинки, текст) можно представить, как комбинацию нескольких крупных сигналов и большое кол-во более мелких. Здесь работает такая аналогия - когда художник рисует каритину, он сначала обозначает горизонт, крупными мазками закрашивает небо, землю, деревья, затем добавляет объекты среднего размера, и уже затем прорабатывает все мелкие детали. Если мы научимся декомпозировать картину на составляющие ее сигналы, то сможем на нее смотреть с любым уровнем детализации.\n",
    "\n",
    "По такому принципу работает поиск похожих картинок в Google - алгоритм не сравнивает картинки попиксельно, он сначала определяет семантику, примерное содержание изображенного на картинке, а уже потом ищет в своей базе картинки с похожим смысловым наполнением.\n",
    "\n",
    "По этому же принципу работает, например, Google переводчик. Он не лезет в словарь за переводом каждого слова, он сначала определяет смысл (семантику) сказанного, а уже потом преобразует этот смысл в текст на другом языке.\n",
    "\n",
    "То есть это все методы, заточенные на определение семантики (смыслового содержание) сигнала, а не его описания.\n",
    "\n",
    "Технически почти все эти алгоритмы реализованы так - они ищут некое сокращенное описание сигнала, которое сохраняет максимум информации о нем. Это сокращенное (латентное) описание и есть семантика сигнала.\n",
    "\n",
    "#### Применительно к текстам\n",
    "\n",
    "Нам хочется, чтобы текст представлялся не как набор слов, а как комбинация небольшого числа осмысленных характеристик. Например, \"текст об искусстве; речь от первого лица; есть ссылки на факты\". \n",
    "\n",
    "Простые модели (SVD, pLSA, LDA) позволяют выделять тематики, составляющие текст. Более сложные модели, основанные на нейронных сетях, опредляют не только тематику, но могут смотреть глубже: например, ведется ли речь от первого лица, сколько в повествовании главных геороев и прочее. На этом занятии ограничимся рассмотрением тематических моделей.\n",
    "\n",
    "В математике есть понятие \"спектральное разложение матриц\". Суть в том, что любую матрицу можно выразить как произведение других матриц, причем точность приближения можно выбирать любую, просто варьируя размерность соствляющих матриц.\n",
    "\n",
    "В нашем случае матрица - это Document-Term матрица, описывающая наши тексты. Если мы ее разложим в произведение более простых матриц, получим скоращенное описание текстов. Такое разложение выделяет только значимый сигнал, отсеивая малозначимый шум. По этой причине процесс описанный в этом разделе, ещё иногда называют низкочастотной фильтрацией. \n",
    "\n",
    "К сожалению мы не знаем, как именно интерпретируются элементы сокращенного описания, но частично можем ответить на этот вопрос, выведя топ слов, обладающих наибольшим весом в описании каждого элемента (тематики).\n",
    "\n",
    "Таким образом, на выходе имеем:\n",
    "- описание найденных в корпусе тематик через топ слова\n",
    "- представлем все документы в виде векторов размерности сильно меньшей оригинальной => это позволяет эффективно использовать эти данные при дальнейших манипуляциях с текстами: решения задач классификации, кластеризации, поиска дубликатов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, какие есть методы разложения\n",
    "\n",
    "### Singular-Value Decomposition\n",
    "\n",
    "Взятое из математики матричное сингулярное разложение (SVD) очень популярно в машинном обучении. Оно раскладывает произвольную прямоугольную матрицу в произведение 3 матриц. \n",
    "\n",
    "<img src=\"img/svd.png\" width=500>\n",
    "\n",
    "Причем это разложение можно делать приближенным, просто уменьшая ранг **r** - вторую размерность матриц U и V. \n",
    "- Если оставляем r=n, то имеем дело с обычным равенством\n",
    "- Если ставим r меньше, равенство заменяется на приближенное равенство\n",
    "- Чем меньше ставим ранг, тем грубее получается приближение, но зато тем проще можем описывать наши тексты\n",
    "\n",
    "SVD - удобный интсрумент получения сокращенного латентного описания текстов\n",
    "\n",
    "Зачем вообще это нужно:\n",
    "1. получаем отдельное представление для \"строк\" и \"столбцов\" матрицы A\n",
    "    \n",
    "        применительно к нашей задаче строки - тексты, столбцы - слова => получаем сокращенное описание текстов и сокращенное описание слов\n",
    "\n",
    "\n",
    "2. можем выбрать любую размерность k получаемого представления \"строк\" и \"столцов\", обычно k << n\n",
    "\n",
    "        чем меньше k, тем \"грубее\" описание каждого текста, но тем более оно сжатое =>\n",
    "            - имея компактные описания мы можем более эффективно работать с данными\n",
    "            - тем меньше в описаниях случайного шума, а шум в задачах ML - это плохо\n",
    "    \n",
    "3. перемножая матрицы обратно, можем приблизить исходную матрицу с любой наперед выбранной точностью\n",
    "\n",
    "        и тем самым можем, например, оценить, насколько слово соотвествует данному документу, несмотря на то, что в орининальном тексте этого слова нету"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TruncatedSVD\n",
    "В библиотеке sklearn за это отвечает класс TruncatedSVD. Здесь преобразование называется \"Truncated\", потому что его смысл в получении описаний меньшего размера (r < n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основной параметр: \n",
    "- n_components -  ранг разложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разложение выполняется методом fit_transform()\n",
    "\n",
    "На выходе получаем уменьшенное описание документа в виде неинтерпретируемого вектора фиксированной размерности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectorized_text = svd.fit_transform(vectorized_text)\n",
    "reduced_vectorized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_vectorized_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы устанавливали размерность равную 10, видим, что матрица трансформировалась в матрицу размерности (1741 x 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_load = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "text_dataset = fetch_20newsgroups(subset='train', categories = categories_to_load, shuffle=True)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.05, max_df=0.5)\n",
    "vectorized_text = vect.fit_transform(text_dataset['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic LSA (pLSA)\n",
    "Модификацией описанного выше подхода LSA является \"Probabilistic LSA\" (pLSA). Здесь пытаемся описать с помощью вероятностной модели.\n",
    "\n",
    "В рамках этой модели каждое слово генерируется с некоторой веротяностью. \n",
    "1. Вероятностное распределение слова зависит от выбранной для этого слова тематики \n",
    "2. Тематика слова зависит от того, что это за документ (в каждом документе свое распределение тематик)\n",
    "\n",
    "### Latent Dirichlet Allocation (LDA)\n",
    "В своей базовой постановке метод pLSA почти не используется. Наиболее часто встречающаяся реализация pLSA - это метод Latent Dirichlet Allocation (LDA). Называется так потому, что в качестве распределения тематики в документе P(c|d) и слова в тематике P(w|c) используются [распределения Дирихле](https://en.wikipedia.org/wiki/Dirichlet_distribution).\n",
    "\n",
    "### Реализация в Sklearn\n",
    "\n",
    "В Sklearn для этого есть класс LatentDirichletAllocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=4,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы явно указываем, что в тексте ожидаем 4 темы (n_components=4). В результате хотим увидеть список тем, каждую как взвешенную сумму термов.\n",
    "\n",
    "Обучение происходит методом fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.fit(vectorized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Атрибут components_ возвращает веса кадого терма для каждого топика:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы как-то описать выделенные тематики, выведем по несколько слов, наиболее ассоциированных с каждой из них:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(lda, vect.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В описаниях какие-то тематики угадываются легко, какие-то не очень. \n",
    "\n",
    "Обратите внимание, что тематики могут выводиться в произвольном порядке между разными запусками модели, поэтому не стоит их называть по номеру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_load = ['rec.autos','rec.motorcycles']\n",
    "text_dataset = fetch_20newsgroups(subset='train', categories = categories_to_load)\n",
    "\n",
    "vect = TfidfVectorizer(min_df=0.05, max_df=0.25, ngram_range=(1,1), stop_words='english', use_idf=False)\n",
    "vectorized_text = vect.fit_transform(text_dataset['data'])\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.)\n",
    "\n",
    "lda.fit(vectorized_text)\n",
    "\n",
    "print_top_words(lda, vect.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Математика методов Topic Modeling  (для интересующихся)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим систему из нескольких случайных величин, какие-то из которых могут зависеть друг от друга.\n",
    "\n",
    "Вероятностная графическая модель - способ визуализации подобных зависимостей в виде графа.\n",
    "\n",
    "Например, здесь представлены 5 случайных величин, но каждая зависит от другой. Оценка (Grade) зависит от сложности экзамена (Difficulty) и успеваемости студента (Intelligence). Средний бал (SAT) зависит от успеваемости в течение года. А наличие рекомендательного пиьсма (Letter) зависит от оценок за экзамен.\n",
    "\n",
    "<img src=\"img/probabilistic.jpeg\" width=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plate Notation\n",
    "\n",
    "Plate Notation - удобный способ схематичного изображения графических веротяностных моделей, которые содержат большое число повторяющихся элементов.\n",
    "\n",
    "Цель: удобство восприятия модели\n",
    "\n",
    "Как читается Plate Notation?\n",
    "- Стрелка - наличие зависимости (переменная в которую идет стрелка зависит от первой переменной)\n",
    "- Закарашенные круги - наблюдаемые переменные\n",
    "- Пустые - латентные переменные\n",
    "- Прямоугольник с числом N внизу - означает \"повторяется N раз\"\n",
    "\n",
    "Проиллюстрируем Plate Notation на примере тематической модели pLSA\n",
    "\n",
    "<img src=\"img/plate0.png\" width=300>\n",
    "\n",
    "\n",
    "\n",
    "У нас есть M документов (внешняя плашка) и в каждом таком документе N слов (внутренняя). \n",
    "\n",
    "С каждым словом связана латентная переменная c, обозначающая тематику. Веротяность тематики зависит от документа: P(c|d), так как каждый документ это какой-то свой микс из нескольких тематик.\n",
    "\n",
    "А распределение конкретного слова w зависит от выбранной тематики P(w|c).\n",
    "\n",
    "**Задача pLSA:** найти конкретные распределения P(c|d) и P(w|c).\n",
    "\n",
    "У нас есть корпус текстов, поэтому подбираем наиболее правдоподобную модель (распределения) мы методом максимального правдоподобия.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Distribution\n",
    "\n",
    "Распределение Дирихле  - вероятностное распределение k-элементного вектора $x=(x_1,x_2 \\dots x_k)$ при условии $\\sum{x_i}=1$.\n",
    "\n",
    "В математике множество векторов ($x_1 ... x_k$), где $\\sum{x_i}=1$ называют <a href=\"https://ru.wikipedia.org/wiki/%D0%A1%D0%B8%D0%BC%D0%BF%D0%BB%D0%B5%D0%BA%D1%81\">симплексом</a>, он же (k-1) -мерный треугольник. \n",
    "\n",
    "То есть распределение Дирихле - это просто распределение вероятности на неком гипертреугольнике. На картинке ниже пример для k=3:\n",
    "<img src=\"img/dirichlet.png\" width=350>\n",
    "\n",
    "Плотность распределния Дирихле выражается через произведение координат $x_i$, нормированное Бета-функцией: $$f \\left(x_1,\\ldots, x_{K}; \\alpha_1,\\ldots, \\alpha_K \\right) = \\frac{1}{\\mathrm{B}(\\boldsymbol\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_i - 1}$$\n",
    "\n",
    "### Параметры распределения\n",
    "\n",
    "У распределения Дирихле есть параметры $\\hat{\\alpha} = (\\alpha_1, \\alpha_2, \\dots \\alpha_k)$, задающие конкретный вид распределения: $\\alpha_i > 0$.\n",
    "\n",
    "Посмотрим, как меняется распределение при разных параметрах. Для наглядности возьмем k=3.\n",
    "\n",
    "|$\\bar{\\alpha}$|распределение|\n",
    "|---|---|\n",
    "|(1,1,1) | равномерное распределение |\n",
    "|(0.2,0.2,0.2) | больше вероятности у краев (когда часть $x_i$ зануляется)|\n",
    "|(7,7,7) | больше распределения у центра |\n",
    "|(5,5,2)| сдвигается к одной из вершин |\n",
    "\n",
    "<img src=\"img/dir1.png\" width=300>\n",
    "\n",
    "Давайте посэмплируем точки из трех распределений Дирихле:\n",
    "- (0.1,0.1,0.1)\n",
    "- (1,1,1)\n",
    "- (4,4,4)\n",
    "\n",
    "Получится примерно следующее:\n",
    "\n",
    "<img src = \"img/dir2.png\" siwth=250>\n",
    "\n",
    "То есть видим, что распределение Дирихле позволяет довольно гибко моделировать различные смеси тематик / смеси слов, поэтому и стали использовать в методе.\n",
    "\n",
    "-----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В рамках LDA используется байесовский способ подбора параметров модели. Это отдельная сложная тема. Суть: сначала мы выбраем некое априорное распределение для параметров (простое), а затем итеративно корректируем его с учетом наблюдаемых данных выборки.\n",
    "\n",
    "В качестве априорных распределений для $\\alpha$ и $\\beta$ используются также распределения Дирихле. Причем используется симметричное распределение \n",
    "- $\\alpha = \\alpha_1 = \\alpha_2 = \\dots = \\alpha_k$ \n",
    "- $\\beta = \\beta_1 = \\beta_2 = \\dots = \\beta_k$\n",
    "\n",
    "LDA в plate нотации выглядит так: \n",
    "<img src=\"img/lda.png\">\n",
    "\n",
    "Как с точки зрения модели пошагово выглядит процесс, генерирующий корпус текстов?\n",
    "1. Для каждого документа генерируем своё распределение тематик  $\\theta_i \\sim Dir(\\alpha)$\n",
    "2. Для каждой тематики генерирем распределение слов $\\phi_j \\sim Dir(\\beta)$\n",
    "3. Для каждой позиции в документе выбираем тематику $z_{ij} \\sim Multinomial(\\theta_i)$\n",
    "4. Из этой тематики выбираем конкретное слово, которое ставится в эту позицию $w_{ij} \\sim Multinomial(\\phi_{z_{ij}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Меры качества в Topic Modeling\n",
    "\n",
    "### Perplexity \n",
    "\n",
    "Perplexity - среднее правдоподобие модели:\n",
    "\n",
    "$$P=\\exp\\bigg[ -\\frac{\\sum{\\log{P(w|\\alpha,\\beta)}}}{\\sum{N_d}}\\bigg] = \\exp^{-E \\big[ \\log P(w|\\alpha, \\beta) \\big]}$$\n",
    "\n",
    "где $w$ - слово, $\\alpha, \\beta$ - наденные параметры модели, $N_d$ - кол-во слов в документе d, всего M документов\n",
    "\n",
    "Arxiv: <a href=\"http://dirichlet.net/pdf/wallach09evaluation.pdf\">Wallach09</a>\n",
    "\n",
    "$P \\in [0, +\\inf)$\n",
    "\n",
    "- Если имеем полную разделимость по тематикам и средняя $P(w|\\alpha,\\beta) = 1$, то $P = 0$\n",
    "- Если всё перемешано и средняя $P(w|\\alpha,\\beta) = 0$, то $P = 1$\n",
    "\n",
    "Разумеется, perplexity уменьшается с ростом числа топиков, но можно использовать для подбора других параметров:\n",
    "<img src=\"img/perplexity1.png\" width = 500>\n",
    "\n",
    "Здесь например приведена сетка из 2 параметров модели LDA: num_topics и learning_decay\n",
    "\n",
    "### Coherence\n",
    "\n",
    "Когерентность тематик - группа метрик, считающих \"целостность\" топика по словам, его сотавляющим.\n",
    "\n",
    "Первые метрики были просто расстоянием до плохих topic-word распределений (junk topics). К плохим тематикам можно отнести  те, в которых:\n",
    "- все слова равноверотяны\n",
    "- распределение слов в тематике совпадает с распределением во всем корпусе текстов\n",
    "- тематика распределена равноверотяно по всем документам\n",
    "\n",
    "Метрики качества топиков можно разделить на\n",
    "- Extrinsic - которые требуют внешнего датасета для расчета\n",
    "- Intrinsic - не требуют\n",
    "\n",
    "Наиболее распространены две метрики: \n",
    "- UMass (intrinsic)\n",
    "- UCI (extrinsic)\n",
    "\n",
    "Обе сначала считаются для каждой пары слов (pairwise), составляющих тематику, а затем суммируются:\n",
    "\n",
    "$$Coherence = \\sum_{i<j}{score(w_i,w_j)}$$\n",
    "\n",
    "Arxiv: <a href=\"http://dirichlet.net/pdf/mimno11optimizing.pdf\">Wallach09</a>\n",
    "\n",
    "UCI - это Pointwise Mutual Information для пары слов w_i и w_j\n",
    "\n",
    "$$UCI = \\log{\\frac{P(w_i,w_j)}{P(w_i) \\cdot P(w_j)}}$$\n",
    "\n",
    "UMass\n",
    "\n",
    "$$UMass = \\log{\\frac{P(w_i,w_j)+1}{P(w_j)}}$$\n",
    "\n",
    "В Gensim для расчета когерентности топика используют более продвинутый подход, основанный на алгоритме описанном в <a href=\"http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\">WSDM_Topic_Evaluation</a>\n",
    "\n",
    "<img src=\"\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "[Topic Modeling Review](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05) - good article on medium\n",
    "\n",
    "[LDAviz](https://github.com/TomasKeller/python-LDAviz) - nice python library for topic modeling with good visualization capabilities, almost no tuning requrired\n",
    "\n",
    "[Machinelearningmastery](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/) - simple example of text generation using neural networks and very good blog on ML\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
