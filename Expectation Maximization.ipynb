{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expectation Maximization** - алгоритм построения параметрической вероятностной модели, основанный на итеративном подборе ее параметров\n",
    "\n",
    "## Мотивация\n",
    "\n",
    "Для начала вспомним классическую задачу MLE\n",
    "$$\\Theta_{opt} = argmax_{\\Theta} [ logP(X|\\Theta)]$$\n",
    "\n",
    "Здесь нам нужно найти параметры модели, лучше всего объясняющие наблюдаемые данные\n",
    "\n",
    "Но с расчетом возникает сложность, если $P(X|\\Theta)$ зависит не только от параметров модели, но и от скрытых (латентных) переменных: $P(X|\\Theta) = P(X|Z, \\Theta)$\n",
    "\n",
    "В этом случае нам нужно взвешивать (формулой полной веротяности) по возможным значениям Z $$P(X|\\Theta) = \\sum_{Z} P(X|Z,\\Theta) P (Z|\\Theta)$$\n",
    "\n",
    "## Алгоритм (математически)\n",
    "1. Считаем $$Q(\\Theta|\\Theta_{current}) = E_{Z|X,\\Theta_{current}} logP(X|Z,\\Theta)$$\n",
    "2. Максимизируем $$\\Theta_{new} = argmax_{\\Theta} Q(\\Theta)$$\n",
    "Повторяем до сходимости $$\\Theta_0, \\Theta_1 \\cdots \\Theta_n \\rightarrow \\Theta_{opt}$$\n",
    "\n",
    "-----\n",
    "\n",
    "Важно пояснить нотацию:\n",
    "$Q = E_{Z|X,\\Theta_{current}} logP(X|Z,\\Theta)$ - это взвешенное среднее, то есть \n",
    "- $\\sum P(Z|X,\\Theta) logP(X|Z,\\Theta)$ для дискретных латентных переменных\n",
    "\n",
    "\n",
    "- $\\int P(Z|X) logP(X|Z,\\Theta) d \\Theta$ для непрерывных\n",
    "\n",
    "\n",
    "\n",
    "Также важно отметить, что $\\Theta_{current}$ - это фиксированное значение параметра (результат предыдущей итерации алгоритма), а $\\Theta$ - то, по чему на втором шаге мы оптимизируем. Отсюда нотация $Q(\\Theta|\\Theta_{current})$\n",
    "\n",
    "\n",
    "## Алгоритм (\"на пальцах\"):\n",
    "1. Фиксируем параметры модели $\\Theta_{current}$ и ищем наиболее вероятный набор Z\n",
    "2. Фиксируем значения $Z$ и подбираем наиболее веротяные значения параметра $\\Theta$\n",
    "\n",
    "Легче всего алгоритм иллюстрируется на примере кластеризации k-means или гауссовых смесей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Приложения\n",
    "- K-means\n",
    "- Gaussian Mix Models\n",
    "- Topic Modeling\n",
    "- Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретическая обоснованность алгоритма\n",
    "\n",
    "Функцию правдоподобия можно оценить снизу:\n",
    "\n",
    "\n",
    "$$L(\\theta) = logP(X|\\theta) \\ge E_{Z|X} logP(X,Z) + H(X) = ELBO$$ \n",
    "\n",
    "ELBO = Evidence Lower Bound\n",
    "\n",
    "Схема работы алгоритма показана на картинке ниже:\n",
    "- красная линия - искомая функция правдоподобия (нам не видна)\n",
    "- синяя - ELBO для первой оценки\n",
    "- зеленая - ELBO для второй оценки\n",
    "<img src=\"img/em.jpeg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иллюстрация на примере K-means\n",
    "\n",
    "### Дано \n",
    "Некоторая выборка на плоскости, состоящая из N точек\n",
    "\n",
    "<img src=\"img/kmeans_0.png\">\n",
    "    \n",
    "Принадлежность точки классу определяется латентной переменной $Z=(z_1 \\cdots z_N)$. Считаем, что в ней представлены данные 3 классов и $z \\in \\{'red','blue','green'\\}$. То есть вектор **Z** - это раскраска множества точек выборки в 3 цвета. \n",
    "    \n",
    "Параметры $\\Theta = (\\theta_1, \\theta_2, \\theta_3)$ характеризуют центры распределения каждого из классов $\\{x|x \\in X_1\\},\\{x|x \\in X_2\\},\\{x|x \\in X_3\\}$. Раскраска в k-means однозначно опредлеяется параметрами $\\Theta$.\n",
    "\n",
    "### Задача\n",
    "Определить оптимальный набор $\\Theta$, который наиболее правдоподобно описывает данные. Раскраска (Z) тогда будет рассчитана автоматически и должна выглядеть примерно так:\n",
    "\n",
    "<img src = \"img/kmeans_3.png\">\n",
    "\n",
    "### Алгоритм\n",
    "\n",
    "#### 0. Initialization\n",
    "\n",
    "Берем некий начальный параметр $\\Theta_0$. Значения латентной переменной **Z** зависят от $\\Theta$: чем ближе точка к центру кластера, тем больше веротяность этого кластера $P(Z|\\Theta)$, \n",
    "\n",
    "<img src=\"img/kmeans_1.png\">\n",
    "\n",
    "\n",
    "#### 1 Expectation\n",
    "\n",
    "$E_{Z|X,\\Theta}logP(X,Z) = \\sum P(Z|X,\\Theta_0)logP(X|Z,\\Theta)$\n",
    "\n",
    "- $Z|X\\Theta$\n",
    "\n",
    "    Смотрим условное распределение вероятности латентных переменных Z при фиксированном $\\Theta$: $$P(Z|X,\\Theta)$$\n",
    "\n",
    "    Для начала посчитаем $P(z_i|x_i,\\Theta)$ для отдельной точки - она могла быть сгенерирована из любого из трех классов, поэтому ее приндлежность - апостериорная веротяность, которая рассчитывается по теореме Байеса, как взвешенная сумма правдоподобий каждого класса.\n",
    "\n",
    "    Далее $P(Z|X,\\Theta)$ считается как произведение $P(z_i|x_i,\\Theta)$\n",
    "\n",
    "    Соджержательно это ответ на вопрос - Как распределены значения латентных переменных (разделение на классы) при заданном $\\Theta$\n",
    "    \n",
    "\n",
    "- $log P(X|Z,\\Theta)$\n",
    "\n",
    "    Смотрим правдоподобие выборки $X$ при данных $Z$ и $\\Theta$. То есть при заданной раскраске $Z$ и заданных распределениях $\\Theta$, какова веротяность наблюдать данную выборку.\n",
    "\n",
    "- $E(\\Theta)$\n",
    "\n",
    "    \n",
    "\n",
    "Далее, так как k-means это у нас пример hard-clustering, то мы дальше мы работаем не с распределением, а берем наиболее вероятный набор значений. Он легко определяется без особой математики - всё  что ближе к центру кластера X, относится к кластеру X.\n",
    "\n",
    "<img src=\"img/kmeans_2.png\">\n",
    "\n",
    "\n",
    "#### 2. Maximization \n",
    "\n",
    "Теперь фиксируем Z, который мы нашли на предыдущем шаге, и корректируем параметр $\\Theta$. \n",
    "\n",
    "Выбираем на плоскости такие центры распределения, которые лучше всего описывают текущие значения Z. Лучше всего = правдоподобие $P(X|Z,\\Theta)$ наибольшее.\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иллюстрация на примере GMM (gaussian mix model)\n",
    "\n",
    "Он же soft-clustering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
